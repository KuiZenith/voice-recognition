# AI final project: Voice Recognition

# Introduction

## Our Problem

Our project aims to train a robust acoustic fingerprint recognition model using only a small amount of data. Typically, acoustic fingerprint recognition models require a large amount of data to achieve good performance. However, we leverage the capability of the voice generation model called Bark, which can generate corresponding sounds based on given voices. By using the sounds generated by Bark as our data source, we can achieve the goal of training a high-quality acoustic fingerprint recognition model with minimal data. We then apply this model to real-life voice recognition tasks.

Our acoustic fingerprint recognition model consists of two parts: a small model for determining acoustic similarity and a larger model that serves as a wrapper for classification decisions.

Most of our experiments focus on exploring methods to improve the accuracy of the small model in determining acoustic similarity. To enhance the accuracy of the small model, we conducted various experiments. For example, we compared the use of LSTM and GRU in the RNN part and examined the impact of setting the number of RNN layers to 1 or 2. We also explored different approaches such as using Conv1d with the original audio sequences and Conv2d with Mel Spectrograms to test which methods could achieve higher accuracy with limited data. Finally, we leverage a linear classifier model composed of DNN, ReLU and Dropout layers to output a value that can be understood as the similarity between two given voices. As for the larger model that serves as a wrapper for classification decisions of the small model, we implemented it using Softmax.

æˆ‘å€‘çš„å°ˆæ¡ˆæ˜¯è¦åˆ©ç”¨å°‘é‡çš„è³‡æ–™å°±èƒ½å¤ è¨“ç·´å‡ºè‰¯å¥½çš„è²ç´‹è¾¨è­˜æ¨¡å‹ã€‚ä¸€èˆ¬çš„è²ç´‹è¾¨è­˜æ¨¡å‹éœ€è¦åˆ©ç”¨å¤§é‡çš„è³‡æ–™æ‰èƒ½å¤ è¨“ç·´å‡ºè‰¯å¥½çš„è²ç´‹è¾¨è­˜æ¨¡å‹ï¼Œä½†æ˜¯æˆ‘å€‘é€éäººè²ç”Ÿæˆæ¨¡å‹ Bark èƒ½å¤ ä½¿ç”¨çµ¦å®šçš„è²éŸ³ç”Ÿæˆå‡ºå°æ‡‰çš„è²éŸ³çš„é€™å€‹ç‰¹æ€§ï¼Œåˆ©ç”¨ Bark æ‰€ç”Ÿæˆçš„è²éŸ³ç•¶ä½œè³‡æ–™ä¾†æºï¼Œé”æˆåªéœ€è¦å°‘é‡çš„è³‡æ–™å°±èƒ½å¤ è¨“ç·´å‡ºè‰¯å¥½çš„è²ç´‹è¾¨è­˜æ¨¡å‹çš„ç›®æ¨™ï¼Œä¸¦ä¸”å°‡é€™å€‹æ¨¡å‹å¥—ç”¨åˆ°çœŸäººçš„è²éŸ³è¾¨è­˜ä¸Šã€‚æˆ‘å€‘çš„è²ç´‹è¾¨è­˜æ¨¡å‹ç¸½å…±åˆ†æˆå…©å€‹éƒ¨åˆ†ï¼Œåˆ†åˆ¥æ˜¯åˆ¤å®šè²éŸ³ç›¸ä¼¼åº¦çš„å°æ¨¡å‹ä»¥åŠä½œç‚ºåŒ…è£å™¨ï¼ˆwrapperï¼‰é€²è¡Œåˆ†é¡åˆ¤å®šçš„å¤§æ¨¡å‹ã€‚

æˆ‘å€‘æ‰€åšçš„å¯¦é©—å¤§å¤šæ˜¯åœ¨å˜—è©¦æœ‰ä»€éº¼æ–¹æ³•å¯ä»¥è®“åˆ¤å®šè²éŸ³ç›¸ä¼¼åº¦çš„å°æ¨¡å‹èƒ½å¤ é”æˆæ›´å¥½çš„æ­£ç¢ºç‡ã€‚ç‚ºäº†è®“å°æ¨¡å‹èƒ½å¤ æœ‰æ›´é«˜çš„æº–ç¢ºç‡ï¼Œæˆ‘å€‘åšäº†è¨±å¤šå¯¦é©—ï¼Œåƒæ˜¯åœ¨ RNN çš„éƒ¨åˆ†æ¯”è¼ƒäº†ä½¿ç”¨ LSTM å’Œ GRUï¼Œä»¥åŠæ¯”è¼ƒ RNN çš„å±¤æ•¸è¨­ç½®ç‚º 1 å’Œ 2 å°æ–¼çµæœçš„å·®åˆ¥ï¼Œé‚„æœ‰ä½¿ç”¨åŸå§‹è²éŸ³åºåˆ—çš„ Conv1d å’Œä½¿ç”¨ Mel Spectrogram çš„ Conv2d ç­‰å¤šç¨®æ–¹æ³•ï¼Œä¾†æ¸¬è©¦å“ªäº›æ–¹æ³•å¯ä»¥é”æˆåœ¨åªæœ‰å°‘é‡è³‡æ–™çš„æƒ…æ³ä¸‹ï¼Œå°±èƒ½å¤ è¨“ç·´å‡ºé«˜æº–ç¢ºåº¦çš„æ¨¡å‹ã€‚æœ€å¾Œå†é€éç”± DNNã€ReLU å’Œ Dropout çµ„æˆçš„ç·šæ€§æ¨¡å‹ï¼Œè¼¸å‡ºä¸€å€‹å¯ä»¥è¢«è¿‘ä¼¼ç†è§£ç‚ºè²éŸ³ç›¸ä¼¼åº¦çš„æ•¸å€¼ã€‚è‡³æ–¼ä½œç‚ºå°æ¨¡å‹çš„åŒ…è£å™¨ï¼ˆwrapperï¼‰é€²è¡Œåˆ†é¡åˆ¤å®šçš„å¤§æ¨¡å‹ï¼Œæˆ‘å€‘ä½¿ç”¨ Softmax ä¾†é€²è¡Œå¯¦ä½œã€‚

# Related work

- **[Speech recognition with deep recurrent neural networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6638947)**
- ****[Convolutional Neural Networks for Speech Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6857341)****
- ****[Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461368)****

---

- ç¬¬ä¸€ç¯‡åœ¨è¬› RNN åœ¨è²éŸ³è¾¨è­˜é ˜åŸŸçš„æ‡‰ç”¨ï¼Œæˆ‘å€‘åƒè€ƒäº†ä»–å€‘ LSTM å’Œ é›™å‘ RNN çš„æƒ³æ³•ã€‚
- ç¬¬äºŒç¯‡åœ¨è¬› CNN åœ¨èªéŸ³è¾¨è­˜é ˜åŸŸçš„æ‡‰ç”¨ï¼Œæˆ‘å€‘åƒè€ƒäº†ä»–å€‘ä½¿ç”¨é »è­œåœ–å’Œ CNN æ¶æ§‹çš„æƒ³æ³•ã€‚
- ç¬¬ä¸‰ç¯‡åœ¨è¬›ä½¿ç”¨ Mel Spectrogram çš„èªéŸ³åˆæˆï¼Œæˆ‘å€‘åƒè€ƒäº†ä»–å€‘ä½¿ç”¨ Mel Spectrogram çš„æƒ³æ³•ã€‚

# Dataset/Platform

The training data we used was obtained through a voice generation model called Bark. Bark is a model that takes text input and provides corresponding voice samples as output. In our implementation, we first used Bark to generate five distinct male voices and five distinct female voices by inputting a text paragraph. We then further generated more voice segments using these ten different voices. However, due to hardware limitations, we were only able to generate 30 segments for each voice in our implementation.

For testing the voice recognition with real human voices, we sourced the data from a Talk at Google interview video on YouTube titled "Guardians of the Galaxy." We collected voice segments from Chris Pratt and Vin Diesel. These voice clips were gathered to conduct real-world tests and evaluate whether our model can accurately distinguish between voices generated by Bark and voices of real individuals.

æˆ‘å€‘ä½¿ç”¨çš„è¨“ç·´è³‡æ–™æ˜¯é€éäººè²ç”Ÿæˆæ¨¡å‹â€”â€”Bark ç²å¾—çš„ã€‚Bark æ˜¯ä¸€å€‹èƒ½å¤ è¼¸å…¥æ–‡å­—æ®µè½å’Œæä¾›çš„è²éŸ³ä¾†ç”Ÿæˆè²éŸ³ç‰‡æ®µçš„æ¨¡å‹ã€‚åœ¨æˆ‘å€‘çš„å¯¦ä½œä¸­ï¼Œæˆ‘å€‘å…ˆé€éè¼¸å…¥ä¸€ä¸²æ–‡å­—è®“ Bark ç”Ÿæˆäº”ä½ä¸åŒçš„ç”·æ€§è²éŸ³ä»¥åŠäº”ä½ä¸åŒçš„å¥³æ€§è²éŸ³ï¼Œä¸¦é€éé€™åä½ä¸åŒçš„è²éŸ³å†é€²ä¸€æ­¥ç”Ÿæˆå‡ºæ›´å¤šçš„è²éŸ³ç‰‡æ®µã€‚ä½†æ˜¯ç”±æ–¼æˆ‘å€‘çš„ç¡¬é«”é™åˆ¶ï¼Œæˆ‘å€‘çš„å¯¦ä½œä¸­åªæœ‰å°‡æ¯å€‹äººè²å„ç”Ÿæˆå‡º 30 å€‹ç‰‡æ®µã€‚

è‡³æ–¼æˆ‘å€‘ç”¨æ–¼æ¸¬è©¦çœŸäººçš„è²éŸ³è¾¨è­˜çš„è³‡æ–™ä¾†æºæ˜¯å¾ Talks at Google çš„ Youtube ä¸­çš„ä¸€éƒ¨è¨ªè«‡å½±ç‰‡ Guardians of the Galaxyï¼Œè’é›†äº†Chris Pratt ä»¥åŠ Vin Diesel çš„è²éŸ³ç‰‡æ®µã€‚æ”¶é›†é€™äº›è²éŸ³ç‰‡æ®µæ˜¯ç‚ºäº†å¾ŒçºŒçš„çœŸäººæ¸¬è©¦ä¸Šï¼Œç”¨ä¾†æ¸¬è©¦æˆ‘å€‘çš„æ¨¡å‹æ˜¯ä¸æ˜¯èƒ½å¤ æº–ç¢ºåˆ¤æ–·ä¸æ˜¯ç”±Barkæ‰€ç”Ÿå‡ºä¾†çš„è²éŸ³ï¼Œè€Œæ˜¯çœŸäººçš„è²éŸ³ã€‚

# Baseline

## VoiceDataset

VoiceDataset is the preprocessing part and sampling in our implementation. It inherited the Dataset class from torch.utils.data.

For the preprocessing part, it will read voice file from the disk under specific directories, and maybe do some preprocessing depends on whether Conv1d or Conv2d is used.

As for the sampling part, it will randomly pick a target, and generates one-hot encoding labels corresponding to the chosen voice. Then it will iterate all voices in the dataset to return sample-target pairs to use later but were stored in the different list now.

VoiceDataset æ˜¯æˆ‘å€‘å¯¦ä½œä¸­ç”¨ä¾†å°è³‡æ–™é€²è¡Œé å…ˆè™•ç†ä»¥åŠæ¡æ¨£çš„ä¸€å€‹Classã€‚å®ƒç¹¼æ‰¿äº† torch.utils.data ä¸­çš„ Dataset Classã€‚

åœ¨è³‡æ–™çš„é å…ˆè™•ç†éƒ¨åˆ†ï¼Œå®ƒæœƒå¾ç‰¹å®šç›®éŒ„ä¸‹è®€å–è²éŸ³æ–‡ä»¶ï¼Œä¸¦æ ¹æ“šæ˜¯å¦ä½¿ç”¨ Conv1d æˆ– Conv2d é€²è¡Œä¸€äº›é å…ˆè™•ç†çš„æ“ä½œã€‚

è‡³æ–¼æ¡æ¨£éƒ¨åˆ†ï¼Œå®ƒå…ˆæœƒéš¨æ©Ÿé¸æ“‡ä¸€å€‹ç›®æ¨™ï¼Œç„¶å¾Œç”Ÿæˆå°æ‡‰æ‰€é¸è²éŸ³çš„ one-hot encoding çš„æ¨™ç±¤ã€‚æ¥è‘—ä»–æœƒè·‘éè³‡æ–™é›†ä¸­çš„æ‰€æœ‰è²éŸ³ï¼Œå† return å›æ¨£æœ¬ä»¥åŠç›®æ¨™çš„ pairï¼Œè®“æ¥ä¸‹ä¾†çš„æ¨¡å‹å¯ä»¥ä½¿ç”¨ï¼Œä¸éå®ƒå€‘æœƒå…ˆè¢«åˆ†åˆ¥å„²å­˜åœ¨ä¸åŒçš„åˆ—è¡¨ä¸­ã€‚

```python
class VoiceDataset(Dataset):
  def __init__(self, directory, embedding_dim = EMBEDDING_DIM):
    super().__init__()
    self.k = 1
    self.population = 0
    self.length = 0
    self.embedding_dim = embedding_dim
    self.voices = []
    self.base_map = {}
    for filename in os.listdir(directory):
      if not filename.endswith(".wav"): continue
      self.voices.append([])
      self.population += 1
      folder_name = filename.split(".wav")[0]
      self.base_map[folder_name] = self.population
      for fn in os.listdir(f"./data/bark/{folder_name}"):
        if not fn.endswith(".wav"): continue
        self.voices[-1].append(load_voice(f"./data/bark/{folder_name}/{fn}"))
        self.length += 1

  def __getitem__(self, _):
    samples, targets, labels = [], [], torch.zeros(len(self.voices))
    r_index = random.randint(0, len(self.voices) - 1)
    labels[r_index] = 1
    for person in self.voices:
      for s in random.choices(person, k=self.k):
        r = random.randint(0, len(s) - self.embedding_dim)
        samples.append(s[r:r + self.embedding_dim])
      for t in random.choices(self.voices[r_index], k=self.k):
        r = random.randint(0, len(t) - self.embedding_dim)
        targets.append(t[r:r + self.embedding_dim])
    samples = torch.tensor(np.concatenate(samples)).view(len(self.voices) * self.k, -1).float()
    targets = torch.tensor(np.concatenate(targets)).view(len(self.voices) * self.k, -1).float()
    return samples, targets, labels

  def get_selected(self, target):
    samples, targets = [], []
    for person in self.voices:
      for s in random.choices(person, k=self.k):
        r = random.randint(0, len(s) - self.embedding_dim)
        samples.append(s[r:r + self.embedding_dim])
        r = random.randint(0, len(target) - self.embedding_dim)
        targets.append(target[r:r + self.embedding_dim])
    samples = torch.tensor(np.concatenate(samples)).view(len(self.voices) * self.k, -1).float()
    targets = torch.tensor(np.concatenate(targets)).view(len(self.voices) * self.k, -1).float()
    return samples, targets

  def __len__(self): return self.length
```

## **Judge**

Judge is the main part for training in the architecture of our model design. It consists of two Sequential layers used for passing features in the voices and a LSTM used for RNN. Certain ratio is assigned for Dropout to prevent overfitting. In the end of seq2 , we apply a Sigmoid function to obtain a float between 0 and 1, which is similar to the concept of similarity. When doing forward the output is collected for concatenation and return as a tensor of 1D shape 10.

This is a fully connected CNN/DNN sub-model, and is not customizable.

The reason why we use window size 25ms and step stride 10ms is according to this [video](https://youtu.be/hYdO9CscNes?t=292).

Judge æ˜¯æˆ‘å€‘é€™ä»½ Final Project æœ€ä¸»è¦è¨“ç·´çš„éƒ¨åˆ†ã€‚å®ƒæ˜¯ç”±å…©å€‹ Sequential å±¤çµ„æˆï¼Œç”¨ä¾†å‚³éè²éŸ³ä¸­çš„ç‰¹å¾µï¼Œä»¥åŠä¸€å€‹ç”¨æ–¼ RNN çš„ LSTM å±¤ã€‚æˆ‘å€‘åˆ†é…äº†ä¸€å®šæ¯”ä¾‹çš„ Dropout ä¾†é˜²æ­¢ Overfittingã€‚åœ¨ seq2 çš„æœ€å¾Œï¼Œæˆ‘å€‘åˆ©ç”¨ Sigmoid å‡½æ•¸ä¾†ç²å¾—ä¸€å€‹ä»‹æ–¼ 0 å’Œ 1 ä¹‹é–“çš„ float numberï¼Œé€™å°±åƒæ˜¯ç›¸ä¼¼åº¦çš„æ¦‚å¿µã€‚åœ¨ forward çš„æ™‚å€™ï¼Œè¼¸å‡ºæœƒè¢«æ”¶é›†èµ·ä¾†é€²è¡Œé€£æ¥ï¼Œä¸¦è¿”å›å½¢ç‹€ç‚º 1D çš„ tensorã€‚

é€™æ˜¯ä¸€å€‹ç„¡æ³•è‡ªå®šç¾©çš„å®Œå…¨é€£æ¥ CNN/DNN å­æ¨¡å‹ã€‚

è‡³æ–¼æˆ‘å€‘å°‡ window size è¨­ç½®æˆ 25 æ¯«ç§’ ä»¥åŠ step stride è¨­ç½®æˆ 10 æ¯«ç§’æ˜¯æ ¹æ“šæå®æ¯…æ•™æˆåœ¨ä¸€éƒ¨å½±ç‰‡æ‰€æåˆ°çš„ï¼Œæˆ‘å€‘æœ‰å°‡é€£çµæ”¾åœ¨ PPT ä¸­ã€‚

```python
class Judge(nn.Module):
  def __init__(self, k):
    super().__init__()
    self.k = k
    self.seq1 = nn.Sequential(
      nn.Conv1d(1, 64, int(0.025 * SAMPLE_RATE), int(0.01 * SAMPLE_RATE)),
      nn.BatchNorm1d(64),
      nn.MaxPool1d(4),
      nn.ReLU(True),
      nn.Conv1d(64, 128, 4),
      nn.BatchNorm1d(128),
      nn.MaxPool1d(4),
      nn.ReLU(True),
      nn.Conv1d(128, 256, 4),
      nn.BatchNorm1d(256),
      nn.MaxPool1d(4),
      nn.ReLU(True),
      nn.Flatten(),
      nn.Dropout(0.3)
    )
    self.seq2 = nn.Sequential(
      nn.Linear(256 * 3 * 2, 1024),
      nn.ReLU(True),
      nn.Dropout(0.2),
      nn.Linear(1024, 256),
      nn.ReLU(True),
      nn.Dropout(0.1),
      nn.Linear(256, 64),
      nn.ReLU(True),
      nn.Linear(64, 1),
      nn.Sigmoid(),
    )

  def forward(self, sample, target):
    outputs1 = self.seq1(sample.view(-1, 1, sample.size(1)))
    outputs2 = self.seq1(target.view(-1, 1, target.size(1)))
    outputs = self.seq2(torch.cat((outputs1, outputs2), dim=1))
    outputs = torch.cat([outputs[self.k * i:self.k * (i + 1), :] for i in range(0, outputs.size(0), self.k)], dim=1).sum(0)
    return outputs
```

## Model

Model is a class that is designed for wrapping Judge and Softmax layer for the purpose of handling varying batch size. In the Model part, the batch size is the actual batch size from the batched data given by the dataloader. However, in the Judge part, the batch size is the number of people, which enables the whole model structure to deal with multiple people so that we can avoid modifying the Judge model again and again with only the number of people changed. Instead, by leveraging this method but not hardcoded the number of people into the Judge, we can augment the model easily and maximize its power.

Model æ˜¯ä¸€å€‹ç”¨æ–¼åŒ…è£ Judge å’Œ Softmax å±¤çš„ Classï¼Œç”¨ä¾†è™•ç†ä¸åŒçš„ batch sizeã€‚åœ¨ Model éƒ¨åˆ†ï¼Œbatch size æ˜¯ç”¨ data loader æ‰€æä¾›çš„batched data çš„å¯¦éš›å¤§å°ã€‚ç„¶è€Œï¼Œåœ¨ Judge éƒ¨åˆ†ï¼Œbatch size æ˜¯äººæ•¸çš„æ•¸é‡ï¼Œè®“æ•´å€‹æ¨¡å‹çµæ§‹èƒ½å¤ è™•ç†å¤šäººçš„æƒ…æ³ï¼Œé¿å…å¦‚æœåªæ”¹è®Šäººæ•¸å°±éœ€è¦åè¦†ä¿®æ”¹ Judge æ¨¡å‹ã€‚é€šéåˆ©ç”¨é€™ç¨®æ–¹æ³•ï¼Œè€Œä¸æ˜¯å°‡äººæ•¸ç¡¬å¯«åˆ° Judge ä¸­ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†æ“´å……æ¨¡å‹ä¸¦æœ€å¤§ç¨‹åº¦åœ°ç™¼æ®ä»–çš„åŠŸèƒ½ã€‚

```python
class Model(nn.Module):
  def __init__(self, n, k = 1):
    super().__init__()
    self.n = n
    self.k = k
    self.judge = Judge(k)
    self.softmax = nn.Softmax(dim=1)

  def forward(self, samples, targets):
    outputs = []
    for s, t in zip(samples, targets): outputs.append(self.judge(s, t).unsqueeze(1))
    outputs = torch.cat(tuple(outputs), dim=1).permute(1, 0)
    outputs = self.softmax(outputs)
    return outputs
```

Concatenating the parts mentioned above, we get a model that judges similarity between voices, hence later processes for training can be implemented.

å°‡ä¸Šè¿°æåˆ°çš„å…©å€‹éƒ¨åˆ†æ¥åœ¨ä¸€èµ·ï¼Œæˆ‘å€‘å°±å¾—åˆ°äº†ä¸€å€‹ç”¨æ–¼åˆ¤æ–·è²éŸ³ç›¸ä¼¼åº¦çš„æ¨¡å‹ï¼Œä¸¦ä¸”å¯ä»¥å¯¦ä½œå¾ŒçºŒçš„è¨“ç·´ã€‚

# Main Approach

Our training approach involves using sounds generated by Bark for training purposes. In each training iteration, we randomly sample a specific duration of sound, with a chosen duration of 3 seconds as our input. The sampling process is implemented within the VoiceDataset class, which allows for modifications of the duration as needed.

æˆ‘å€‘çš„è¨“ç·´æ–¹æ³•æ˜¯åˆ©ç”¨ Bark ç”Ÿæˆçš„è²éŸ³ä¾†é€²è¡Œè¨“ç·´ã€‚åœ¨æ¯å€‹è¨“ç·´å‘¨æœŸä¸­ï¼Œæˆ‘å€‘æœƒéš¨æ©ŸæŠ½æ¨£ä¸€æ®µç‰¹å®šçš„è²éŸ³æ™‚é•·ï¼Œè€Œæˆ‘å€‘æ˜¯é¸æ“‡ 3 ç§’ä½œç‚ºè¼¸å…¥ã€‚è‡³æ–¼æŠ½æ¨£éç¨‹æ˜¯åœ¨ VoiceDataset çš„ Class ä¸­å¯¦ç¾çš„ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“šéœ€æ±‚ï¼Œå°æ¡æ¨£çš„æ™‚é•·é€²è¡Œä¿®æ”¹

## Techniques

In our implementation, we utilize LSTM and GRU as the architectures for the RNN component. Their designs enable the model to capture long-term dependencies in sequential data. Next, we will introduce and briefly compare the differences between LSTM and GRU.

åœ¨æˆ‘å€‘çš„å¯¦ä½œä¸­ï¼Œæˆ‘å€‘åˆ©ç”¨ LSTM å’Œ GRU ä½œç‚º RNN çš„æ¶æ§‹ã€‚å®ƒå€‘çš„è¨­è¨ˆä½¿å¾—æ¨¡å‹èƒ½å¤ æ•æ‰åˆ°åºåˆ—è³‡æ–™ä¸­çš„é•·æœŸ dependencyï¼Œæ¥ä¸‹ä¾†æˆ‘å€‘å°‡æœƒä»‹ç´¹ä¸¦ä¸”ç°¡å–®æ¯”è¼ƒä»–å€‘ä¹‹é–“çš„å·®ç•°ã€‚

### LSTM

LSTM consists primarily of a memory cell and three types of gates: the forget gate, the input gate, and the output gate.

The memory cell acts as a pathway for information to flow through. During sequence processing, the memory cell retains relevant information and carries it forward, preserving earlier information.

As for the gates, the forget gate takes the previous hidden state information and the current input and passes them through a sigmoid function, using the output to determine what to forget or remember from the previous information. The input gate also takes the previous hidden state information and the current input, passing them through a sigmoid function to determine their importance and update the cell state. The output gate determines the value of the next hidden state to be used in the future, incorporating relevant information from the previous inputs.

Due to the relatively complex structure of the memory cell and gates in LSTM, it requires more computation time and takes longer to converge during training.

LSTM ä¸»è¦ç”±è¨˜æ†¶å–®å…ƒå’Œä¸‰ç¨®é¡å‹çš„é–€çµ„æˆï¼šéºå¿˜é–€ã€è¼¸å…¥é–€ï¼Œä»¥åŠè¼¸å‡ºé–€ã€‚

è¨˜æ†¶å–®å…ƒæ˜¯ä¸€å€‹è®“è¨Šæ¯å‚³éä¸‹å»çš„é€šè·¯ã€‚åœ¨åºåˆ—è™•ç†éç¨‹ä¸­ï¼Œè¨˜æ†¶å–®å…ƒèƒ½ä¸€ç›´æ”œå¸¶è‘—ç›¸é—œè¨Šæ¯ï¼ŒæŠŠè¼ƒæ—©çš„è¨Šæ¯ä¿ç•™ä¸‹ä¾†ã€‚

è‡³æ–¼é–€çš„éƒ¨åˆ†ï¼Œéºå¿˜é–€æœƒå°‡å…ˆå‰éš±è—ç‹€æ…‹çš„è³‡è¨Šå’Œç•¶å‰è¼¸å…¥çš„ä¿¡æ¯åŒæ™‚è¼¸å…¥åˆ° Sigmoid å‡½æ•¸ï¼Œä¸¦ç”¨è¼¸å‡ºå€¼ä¾†æ±ºå®šæ˜¯å¦è¦éºå¿˜æˆ–è¨˜ä½å…ˆå‰çš„è¨Šæ¯ã€‚è¼¸å…¥é–€ä¹Ÿæœƒå°‡å…ˆå‰éš±è—ç‹€æ…‹çš„è¨Šæ¯å’Œç•¶å‰è¼¸å…¥çš„è¨Šæ¯è¼¸å…¥åˆ° Sigmoid å‡½æ•¸ä¾†æ±ºå®šé‡è¦æ€§ä¸¦ç”¨ä¾†æ›´æ–°è¨˜æ†¶å–®å…ƒçš„ç‹€æ…‹ã€‚è¼¸å‡ºé–€å‰‡æ±ºå®šæœªä¾†ä½¿ç”¨çš„ä¸‹ä¸€å€‹éš±è—ç‹€æ…‹çš„å€¼ï¼Œéš±è—ç‹€æ…‹ä¸­åŒ…å«äº†å…ˆå‰è¼¸å…¥çš„ç›¸é—œä¿¡æ¯ã€‚

ç”±æ–¼ LSTM å…·æœ‰ç›¸å°è¤‡é›œçš„è¨˜æ†¶å–®å…ƒå’Œé–€çš„çµæ§‹ï¼Œæ‰€ä»¥æœƒéœ€è¦èŠ±è²»æ›´å¤šæ™‚é–“è¨ˆç®—ï¼Œåœ¨è¨“ç·´æ™‚ä¹Ÿéœ€è¦æ›´é•·çš„æ™‚é–“æ‰èƒ½æ”¶æ–‚ã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled.png)

<aside>
ğŸ’¡ **LSTM is used to capture the recurrent features presented in the data.**

</aside>

### GRU

Similar to LSTM, but with improved execution speed and reduced memory usage, GRU simplifies the structure of gates. While LSTM has three gates (forget gate, output gate, and input gate), GRU has two gates, namely the update gate and reset gate. The update gate in GRU is similar to the forget gate and input gate in LSTM as it determines what information to discard and what new information to add. The reset gate, on the other hand, determines how much previous information to discard. GRU combines the cell state and hidden state ($h_t$) and computes new information in a different way compared to LSTM.

GRU å’Œ LSTM å¾ˆåƒï¼Œä½†æ˜¯ GRU æé«˜äº†åŸ·è¡Œé€Ÿåº¦ä¸¦ä¸”æ¸›å°‘è¨˜æ†¶é«”çš„ç”¨é‡ã€‚ä»–ç°¡åŒ–äº†é–€çš„çµæ§‹ï¼Œå°‡ LSTM çš„ä¸‰å€‹é–€ç°¡åŒ–æˆå…©å€‹é–€ï¼Œåˆ†åˆ¥æ˜¯æ›´æ–°é–€ (reset gate) èˆ‡é‡ç½®é–€ (update gate)ã€‚æ›´æ–°é–€é¡ä¼¼ LSTM çš„éºå¿˜é–€å’Œè¼¸å…¥é–€ï¼Œå®ƒæ˜¯ç”¨ä¾†æ±ºå®šä¸Ÿæ£„ä»€éº¼ä¿¡æ¯å’Œæ·»åŠ ä»€éº¼æ–°ä¿¡æ¯ã€‚é‡ç½®é–€çš„ä½œç”¨æ˜¯ç”¨ä¾†æ±ºå®šè¦ä¸Ÿæ£„å¤šå°‘å…ˆå‰è³‡è¨Šã€‚å¦å¤– GRU æŠŠå–®å…ƒç‹€æ…‹ (cell state) å’Œéš±è—ç‹€æ…‹ ($h_t$) é€²è¡Œåˆä½µï¼Œè¨ˆç®—æ–°è³‡è¨Šçš„æ–¹å¼ä¹Ÿå’Œ LSTM ä¹Ÿæœ‰æ‰€ä¸åŒã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%201.png)

<aside>
ğŸ’¡ **GRU is used to capture the recurrent features presented in the data.**

</aside>

### Conv1d

The original form of WAV file is a sequence of amplitude, where sequence indexes represent timestamps. Conv1d takes in 1-dimensional sequence of sound and extract the features during training; the input passes through kernel layer and gives an output. In our implementation, the first input is the sequence of sound, where the sample rate is 24000 Hz with a duration of 3 seconds, therefore a size of 72000 in total. This sequence is fed into seq1 for later processing such as ReLU, MaxPool1d, etc.

WAV æª”æ¡ˆçš„åŸå§‹å½¢å¼æ˜¯ä¸€å€‹æŒ¯å¹…åºåˆ—ï¼Œå…¶ä¸­åºåˆ—çš„ index ä»£è¡¨è‘—æ™‚é–“æˆ³è¨˜ã€‚Conv1d æœƒå…ˆæ¥æ”¶ä¸€ç¶­è²éŸ³åºåˆ—ï¼Œä¸¦åœ¨è¨“ç·´éç¨‹ä¸­æå–ç‰¹å¾µã€‚è¼¸å…¥æœƒé€šé kernel å±¤ä¸¦ç”¢ç”Ÿè¼¸å‡ºã€‚åœ¨æˆ‘å€‘çš„å¯¦ä½œä¸­ï¼Œç¬¬ä¸€å€‹è¼¸å…¥æ˜¯è²éŸ³åºåˆ—ï¼Œæ¡æ¨£ç‡æ˜¯ 24000 Hzï¼ŒæŒçºŒæ™‚é–“ç‚º 3 ç§’ï¼Œå› æ­¤ç¸½å¤§å°ç‚º 72000ã€‚é€™å€‹åºåˆ—æœƒå…ˆè¢«å‚³å…¥ seq1 é€²è¡Œå¾ŒçºŒè™•ç†ï¼Œå¦‚ ReLUã€MaxPool1d ç­‰ã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%202.png)

<aside>
ğŸ’¡ **Conv1d is used to capture the sequence features presented in the data amplitude time series.**

</aside>

### Conv2d

Since the feature is not obvious in wave form and the linear form of amplitude and frequencies need a logarithmic form for a better training performance, we adopt Mel Spectrogram, which is a 2-dimensional sequence and requires Conv2d to be fed into the model. Conv2d is similar to Conv1d, while the input, kernel, and output parts are all 2-dimensional. Conv2d is commonly used in image training, and the spectrogram we adopt is also of 2D type, therefore we leverage Conv2d for later processing.

ç”±æ–¼è²éŸ³ç‰¹å¾µåœ¨ Conv1d çš„æ³¢å‹åœ–ä¸­ä¸æ˜é¡¯ï¼Œè€Œä¸”æŒ¯å¹…å’Œé »ç‡éœ€è¦ç”±ç·šæ€§çš„é—œä¿‚è½‰æ›ç‚ºå°æ•¸å½¢å¼ï¼Œæ‰èƒ½å¤ ç²å¾—æ›´å¥½çš„è¨“ç·´æ•ˆæœä»¥åŠè²¼åˆäººé¡çš„å¯¦éš›æ„ŸçŸ¥è²éŸ³æ–¹å¼ï¼Œå› æ­¤æˆ‘å€‘æ¡ç”¨ Mel Spectrogram ä¾†è½‰æ›è³‡æ–™ï¼Œä¸¦é€šé Conv2d è¼¸å…¥æ¨¡å‹ã€‚Conv2d èˆ‡ Conv1d é¡ä¼¼ï¼Œåªæ˜¯è¼¸å…¥ã€kernal å’Œè¼¸å‡ºéƒ¨åˆ†éƒ½æ˜¯äºŒç¶­çš„ã€‚Conv2d é€šå¸¸ç”¨æ–¼åœ–åƒè¨“ç·´ï¼Œè€Œæˆ‘å€‘æ¡ç”¨çš„é »è­œåœ–ä¹Ÿæ˜¯äºŒç¶­æ•¸å€¼è³‡æ–™ï¼Œå› æ­¤æœƒä½¿ç”¨ Conv2d é€²è¡Œå¾ŒçºŒè™•ç†ã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%203.png)

<aside>
ğŸ’¡ **Conv2d is used to capture the area features presented in the converted data Mel Spectrogram.**

</aside>

### Mel Spectrogram

Spectrogram is a conversion from voice to image. Using Fourier transform, it separate the sound with its frequency and amplitude accordingly. However, for a common spectrogram, it has a linear scale upon frequencies and amplitudes, while human beings listen on a logarithmic scale. For example, it is easy for us to tell the difference between a 100 Hz sound and a 200 Hz sound, while it is hard to tell the difference between 10000 Hz and 10100 Hz, even though they both possess a difference of 100 Hz. The reason why we adopt Decibel scale is similar; an amplitude of 0.1 and 1 has only 10 dB in difference, while they are separated far in amplitude format. In this project, we leveraged function in torchaudio to convert WAV file to Mel Spectrogram, where frequencies is filtered with Mel scale and amplitude is filtered with decibel scale. These two scales are adopted so that the voices fit how human beingsâ€™ hearing system actually work.

é »è­œåœ–æ˜¯å°‡è²éŸ³è½‰æ›ç‚ºåœ–åƒçš„æ–¹æ³•ã€‚ä»–æœƒé€šéå‚…ç«‹è‘‰è®Šæ›æŠŠè²éŸ³æŒ‰ç…§é »ç‡å’ŒæŒ¯å¹…é€²è¡Œåˆ†é›¢ã€‚ç„¶è€Œï¼Œå°æ–¼æ™®é€šçš„é »è­œåœ–è€Œè¨€ï¼Œå®ƒåœ¨é »ç‡å’ŒæŒ¯å¹…ä¸Šæ¡ç”¨çš„æ˜¯ç·šæ€§çš„ï¼Œä½†æ˜¯äººé¡å°æ–¼è²éŸ³çš„æ„Ÿå—æ˜¯å°æ•¸é—œä¿‚çš„ã€‚æ¯”æ–¹èªªæˆ‘å€‘å¯ä»¥è¼•é¬†åœ°åˆ†è¾¨å‡º 100 Hz è²éŸ³å’Œ 200 Hz è²éŸ³ä¹‹é–“çš„å·®ç•°ï¼Œä½†å»å¾ˆé›£åˆ†è¾¨å‡º 10000 Hz å’Œ 10100 Hz ä¹‹é–“çš„å·®ç•°ã€‚æˆ‘å€‘åˆ¤æ–·è²éŸ³çš„å¤§å°ä¹Ÿæ˜¯é€éåˆ†è²çš„å¤§å°ä¾†é€²è¡Œåˆ¤æ–·ï¼Œåƒæ˜¯æŒ¯å¹… 0.1 å’Œ 1 ä¹‹é–“é›–ç„¶åªæœ‰ 10 åˆ†è²çš„å·®ç•°ï¼Œä½†åœ¨æŒ¯å¹…æ ¼å¼ä¸Šå»æœ‰å¾ˆå¤§çš„å·®è·ã€‚åœ¨æˆ‘å€‘çš„ Final Project ä¸­ï¼Œæˆ‘å€‘åˆ©ç”¨äº† torchaudio ä¸­çš„å‡½æ•¸å°‡ WAV æ–‡ä»¶è½‰æ›ç‚º Mel Spectrogramï¼Œå…¶ä¸­é »ç‡ä½¿ç”¨ Mel scale è½‰æ›ï¼ŒæŒ¯å¹…å‰‡æ˜¯è½‰æ›æˆåˆ†è²ã€‚æ¡ç”¨é€™å…©ç¨®åˆ»åº¦æ˜¯ç‚ºäº†è®“è²éŸ³èƒ½å¤ æ›´åŠ ç¬¦åˆäººé¡çš„è½è¦ºå¯¦éš›é‹ä½œæ–¹å¼ã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%204.png)

Above is the visualization of Mel Spectrogram. The x-axis represents timestamp in linear form and y-axis represents frequency in logarithmic form. The color of each coordinate of form (time, frequency) is obtained by converting amplitude into decibel format, which is logarithmic.

<aside>
ğŸ’¡ **Mel Spectrogram is used to solve the extremity problems of the amplitude.**

</aside>

### Model Diagram

The numbers on the diagram represent the sizes of the outputs at each layer. In this example, we found that it is the best case scenario. The input for this example is a batch of 2D mel spectrograms. Each case in the batch is individually passed to the judge for similarity calculation. Although there are four judges shown in the diagram, in practice, we only use a single instance of the judge. The four judges in the diagram are included for ease of understanding. The judge consists of four stages, each stage containing multiple layers, with only one layer of LSTM. Each input batch is split into samples and targets, which are sequentially passed through seq1, LSTM, and seq2. Finally, the outputs of the sample and target are combined and passed to seq3, which represents the output of the judge. All the outputs of the judges are then concatenated and fed into softmax to obtain the final output of the model, representing the probability distribution of the model's prediction of the identity of the sound.

In the diagram, seq1 contains various layers related to convolution, such as the convolution itself, max pooling, and ReLU. Seq2 flattens the output of seq1 after passing through LSTM. Seq3 is a discriminator composed of multiple DNNs and ReLU activations, which outputs a similarity score ranging from 0 to 1.

Next, I will discuss the parts of our model that can be adjusted, which are the aspects we modified during our experiments.

- The input can be either a 2D mel spectrogram or a 1D amplitude sequence.
- In Seq1, the choice between using Conv1d or Conv2d depends on the input.
- The LSTM shown in the diagram can be replaced with a GRU, depending on the requirements.
- If RNN is not used, there will be no LSTM and Seq2 in the diagram. Seq3 will be adjusted to become the new Seq2.

åœ–ä¸Šçš„æ•¸å­—ä»£è¡¨è‘—è©²å±¤è¼¸å‡ºçš„å¤§å°ï¼Œé€™å€‹ä¾‹å­æ˜¯æˆ‘å€‘åœ¨æœ€å¾Œç™¼ç¾æ˜¯æœ€å¥½çš„caseï¼Œä»–çš„inputæ˜¯æœ‰batchçš„2d mel spectrogramã€‚æ¥è‘—batchä¸­çš„æ¯ä¸€å€‹caseæœƒåˆ†åˆ¥ä¸Ÿåˆ°judgeè¨ˆç®—ç›¸ä¼¼åº¦ï¼Œåœ–ä¸­é›–ç„¶æœ‰4å€‹judgeï¼Œä½†æ˜¯å¯¦éš›ä¸Šå¯¦ä½œçš„æ™‚å€™åªæœ‰ä½¿ç”¨ä¸€å€‹judgeçš„instanceï¼Œåœ–ä¸Šæœ‰4å€‹judgeåªæ˜¯ç‚ºäº†æ–¹ä¾¿ç†è§£ã€‚judgeè£¡é¢æœƒæœ‰å››å€‹éšæ®µï¼Œæ¯å€‹seqéšæ®µè£¡é¢å¾ˆå¤šå€‹layerï¼Œè€ŒLSTMåªæœ‰ä¸€å±¤layerã€‚æ¯å€‹inputçš„batchæ‹†åˆ†é–‹ä¾†æœƒæœ‰sampleè·Ÿtargetï¼Œä¾åºä¸Ÿåˆ°seq1ã€LSTMã€seq2è£¡é¢ã€‚æœ€å¾Œå†å°‡sampleè·Ÿtargetçš„è¼¸å‡ºä¸€èµ·ä¸Ÿåˆ°seq3è£¡é¢ï¼Œå°±æ˜¯judgeçš„è¼¸å‡ºã€‚æ¥è‘—å°‡æ‰€æœ‰çš„judgeè¼¸å‡ºé€£æ¥å¾Œå‚³å…¥softmaxè£¡é¢ï¼Œå°±å¯ä»¥å¾—åˆ°æ¨¡å‹çš„æœ€çµ‚è¼¸å‡ºï¼Œä»£è¡¨è‘—æ¨¡å‹çŒœæ¸¬é€™å€‹è²éŸ³æ˜¯èª°çš„æ©Ÿç‡åˆ†å¸ƒã€‚

å…¶ä¸­ä¸Šåœ–çš„seq1è£¡é¢åŒ…å«äº†convolutionçš„ç›¸é—œlayerï¼Œåƒæ˜¯convolutionæœ¬èº«ã€maxpoolingã€ä»¥åŠReLUç­‰ï¼Œseq2è£¡é¢å‰‡æ˜¯å°‡seq1ç¶“éLSTMçš„è¼¸å‡ºæ”¤å¹³ï¼Œseq3å‰‡æ˜¯ä¸€å€‹ç”±å¾ˆå¤šå€‹DNNå’ŒReLUçµåˆèµ·ä¾†çš„é‘‘åˆ¥å™¨ï¼Œä»–æœƒè¼¸å‡ºä¸€å€‹ä»‹æ–¼0åˆ°1ä¹‹é–“ä»£è¡¨è‘—ç›¸ä¼¼åº¦çš„æ•¸å­—ã€‚

æ¥ä¸‹ä¾†æ˜¯æœ‰é—œæˆ‘å€‘æ¨¡å‹å¯ä»¥èª¿æ•´çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘åšå¯¦é©—æ¸¬è©¦å“ªå€‹éƒ¨åˆ†èƒ½å¤ è®“æ¨¡å‹æ›´å¥½æœ‰ä¿®æ”¹åˆ°çš„éƒ¨åˆ†ã€‚

- è¼¸å…¥å¯ä»¥é¸2d mel spectrogramæˆ–æ˜¯1dçš„éœ‡å¹…åºåˆ—
- Seq1è£¡é¢å¯ä»¥æ ¹æ“šè¼¸å…¥ä¾†é¸æ“‡æ˜¯ç”¨Conv1dæˆ–æ˜¯Conv2d
- ä¸Šåœ–çš„LSTMä¹Ÿå¯æ ¹æ“šéœ€æ±‚æ”¹æˆGRU
- å¦‚æœæ²’æœ‰ç”¨RNNçš„è©±ï¼Œå°±ä¸æœƒæœ‰ä¸Šåœ–çš„LSTMå’ŒSeq2ï¼ŒSeq3æœƒéè£œæˆæ–°çš„Seq2

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%205.png)

## Experiments

### Training steps

Generally, to avoid overfitting, we should adjust the total training steps according to the quantity and characteristic of data and model. We conduct experiment on different training steps. As the training resources are limited, we adopt 10000 steps after finding that further increasing training steps would not improve the accuracy and might even lead to overfitting.

é€šå¸¸ï¼Œç‚ºäº†é¿å…éæ“¬åˆï¼Œæˆ‘å€‘æœƒéœ€è¦æ ¹æ“šè³‡æ–™çš„æ•¸é‡å’Œç‰¹æ€§ä»¥åŠæ¨¡å‹é€²è¡Œèª¿æ•´ç¸½å…±çš„è¨“ç·´æ­¥é©Ÿã€‚æˆ‘å€‘æœ‰å°ä¸åŒçš„è¨“ç·´æ­¥é©Ÿé€²è¡Œäº†å¯¦é©—ã€‚ä½†æ˜¯ç”±æ–¼è¨“ç·´è³‡æºæœ‰é™ï¼Œæˆ‘å€‘ç™¼ç¾é€²ä¸€æ­¥å¢åŠ è¨“ç·´æ­¥é©Ÿä¸æœƒæé«˜æº–ç¢ºç‡ï¼Œç”šè‡³å¯èƒ½å°è‡´éæ“¬åˆï¼Œæ‰€ä»¥æˆ‘å€‘ä¹‹å¾Œçš„æ™‚å€™éƒ½é¸æ“‡æ¡ç”¨äº† 10000 æ­¥çš„è¨“ç·´æ­¥æ•¸ã€‚

### With RNN vs Without RNN

RNN is used to remember previous or bidirectional information so that the model can consider these factors later on. Compared to architectures that do not use RNN, using an RNN architecture requires longer training time. In the subsequent analysis of the results, we will examine whether incorporating RNN has an impact on the final model performance.

RNN æ˜¯ç”¨ä¾†è¨˜æ†¶å…ˆå‰æˆ–é›™å‘çš„è³‡è¨Šï¼Œä»¥ä¾¿æ¨¡å‹ä¹‹å¾Œå¯ä»¥è€ƒæ…®é€™äº›å› ç´ ã€‚è·Ÿæ²’æœ‰ä½¿ç”¨ RNN çš„æ¶æ§‹ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨ RNN æ¶æ§‹æœƒéœ€è¦æ›´é•·çš„è¨“ç·´æ™‚é–“ã€‚åœ¨æ¥ä¸‹ä¾†çš„çµæœåˆ†æä¸­ï¼Œæˆ‘å€‘æœƒåˆ†æåŠ å…¥ RNN æ˜¯å¦æœƒå°æœ€çµ‚çš„æ¨¡å‹è¡¨ç¾é€ æˆå½±éŸ¿ã€‚

### LSTM vs GRU

LSTM and GRU have different architectural characteristics and convergence behaviors. LSTM has a more complex architecture and is better suited for long-term memory retention, but it may take longer time to converge. On the other hand, GRU has a simpler architecture and converges faster, requiring fewer steps and less data.

LSTM å’Œ GRU åœ¨çµæ§‹çš„è¤‡é›œåº¦å’Œæ”¶æ–‚çš„é€Ÿåº¦ä¸Šæœ‰æ‰€ä¸åŒã€‚LSTM æœ‰æ›´è¤‡é›œçš„çµæ§‹ï¼Œæ¯”è¼ƒé©åˆé•·æœŸè¨˜æ†¶ä¿ç•™ï¼Œä½†å¯èƒ½éœ€è¦æ¯”è¼ƒé•·çš„æ™‚é–“æ‰èƒ½æ”¶æ–‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGRU å…·æœ‰è¼ƒç°¡å–®çš„çµæ§‹ï¼Œæ”¶æ–‚é€Ÿåº¦æ›´å¿«ï¼Œè€Œä¸”éœ€è¦æ­¥æ•¸å’Œè³‡æ–™æ›´å°‘ã€‚

### 1 layer RNN vs 2 layer RNN

Generally, a model using RNN with 2 layers would possess more complexity than the one with 1 layer. In this case, 19890369 for 2 layers and 18313409 for 1 layer. In the conduction of our experiment, weâ€™ll compare the performance of 1 layer and 2 layer, and modify the total training steps in 2 layer model.

ä¸€èˆ¬è€Œè¨€ï¼Œä½¿ç”¨å…©å±¤ RNN çš„æ¨¡å‹æ¯”ä½¿ç”¨ä¸€å±¤çš„æ¨¡å‹æ›´è¤‡é›œã€‚åœ¨æœ¬å°ˆæ¡ˆä¸­ï¼Œå…©å±¤çš„æ¨¡å‹æœ‰ 19890369 å€‹åƒæ•¸ï¼Œè€Œä¸€å±¤çš„æ¨¡å‹å‰‡æœ‰ 18313409 å€‹åƒæ•¸ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘å°‡æ¯”è¼ƒä¸€å±¤å’Œå…©å±¤æ¨¡å‹çš„æ€§èƒ½ã€‚

### Conv1d with original voice vs Conv2d with Mel Spectrogram

According to this [article](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505), since human beings hear voice in a logarithmic scale rather than a linear scale, the difference between 100 Hz and 200 Hz is more significant than that of 10000 Hz and 10100 Hz, amplitude similar. To fix the problem that scalar difference may differ from hearing difference, Mel Spectrogram can be used to fix the problem on both sound frequency and amplitude. In the following part, we conduct experiment on sound converted to Mel Spectrogram and original data, and compare their performances.

æ ¹æ“šé€™ç¯‡[æ–‡ç« ](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505)ï¼Œç”±æ–¼äººé¡è½åˆ°çš„è²éŸ³æ˜¯ä»¥å°æ•¸å°ºåº¦è€Œä¸æ˜¯ç·šæ€§å°ºåº¦ä¾†æ„ŸçŸ¥çš„ï¼Œæ‰€ä»¥ 100 Hz å’Œ 200 Hz ä¹‹é–“çš„å·®ç•°æ¯” 10000 Hz å’Œ 10100 Hz ä¹‹é–“çš„å·®ç•°æ›´æ˜é¡¯ï¼Œå³ä½¿æŒ¯å¹…ç›¸ä¼¼ã€‚ç‚ºäº†è§£æ±ºæ¨™é‡å·®ç•°å¯èƒ½èˆ‡è½è¦ºå·®ç•°ä¸ä¸€è‡´çš„å•é¡Œï¼Œå¯ä»¥ä½¿ç”¨ Mel Spectrogram ä¾†ä¿®æ­£è²éŸ³é »ç‡å’ŒæŒ¯å¹…ä¸Šçš„å•é¡Œã€‚åœ¨æ¥ä¸‹ä¾†çš„éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡å°è½‰æ›ç‚º Mel Spectrogram å’ŒåŸå§‹æ•¸æ“šçš„è²éŸ³é€²è¡Œå¯¦é©—ï¼Œä¸¦æ¯”è¼ƒå®ƒå€‘çš„æ€§èƒ½è¡¨ç¾ã€‚

# Evaluation metrics

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%206.png)

[åŸå§‹è³‡æ–™é›† æ¨¡å‹è¡¨ç¾](https://www.notion.so/a7aadc1d1f104ff6a2b5c1a5440d8656?pvs=21)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%207.png)

[äººè²è³‡æ–™é›†ï¼ˆ6äººï¼‰ æ¨¡å‹è¡¨ç¾](https://www.notion.so/74328254e2ed425f83df23c7837b0476?pvs=21)

## Validation

Below is the introduction to how we validate the accuracy of the model. To avoid possible biased output that may happen when running insufficient times, we run the same voice on model repeatedly. For each person that the model can specify, we run 1000 times on the voice as input of model for validation. Among all the outputs of these 1000 tests, argmax is leveraged to vote for the most convincing consequence, and set as output.

Our model will ultimately output a graph like this. The "occurrence" on the graph represents the occurrences of the model's predictions for this person's voice among the 1000 outputs. The "test index" indicates the total number of individuals, which in this case is 6. In this graph, it shows that there are 6 individuals.

As for the colors, red represents the person to whom the voice segment belongs, while blue represents voices that do not belong to that person. Therefore, in this graph, it indicates that the model's prediction is correct. The highest voted result by the model is index 0, which corresponds to the red color, representing the true voice owner. This means that in this particular test, the model was able to correctly predict the result.

ä»¥ä¸‹ç‚ºæˆ‘å€‘é©—è­‰æ¨¡å‹æº–ç¢ºæ€§çš„æ–¹æ³•ä»‹ç´¹ã€‚ç‚ºäº†ç›¡é‡é¿å…å› åŸ·è¡Œæ¬¡æ•¸ä¸è¶³è€Œç”¢ç”Ÿçš„è¼¸å‡ºèª¤å·®ï¼Œæˆ‘å€‘æœƒé‡è¤‡åœ¨æ¨¡å‹ä¸Šè¼¸å…¥ç›¸åŒçš„èªéŸ³ã€‚å°æ–¼æ¯å€‹èƒ½è¢«æ¨¡å‹è­˜åˆ¥å‡ºçš„äººï¼Œæˆ‘å€‘æœƒæŠŠä»–çš„èªéŸ³é‡è¤‡è¼¸å…¥æ¨¡å‹1000æ¬¡ä»¥é€²è¡Œé©—è­‰ã€‚æœ€å¾Œï¼Œæˆ‘å€‘å¾é€™1000æ¬¡æ¸¬è©¦çš„è¼¸å‡ºä¸­ï¼Œåˆ©ç”¨argmaxä¾†æŠ•ç¥¨é¸å‡ºæœ€æœ‰èªªæœåŠ›çš„çµæœï¼Œä¸¦å°‡å…¶è¼¸å‡ºã€‚

èˆ‰ä¾‹ä¾†èªªï¼Œæˆ‘å€‘çš„æ¨¡å‹æœ€çµ‚æœƒè¼¸å‡ºé€™ç¨®åœ–ç‰‡ï¼Œåœ–ä¸Šçš„ occurrence ä»£è¡¨è‘—é€™1000æ¬¡çš„è¼¸å‡ºä¸­ï¼Œæ¨¡å‹è¼¸å‡ºæ‰€çŒœæ¸¬æ˜¯é€™å€‹äººçš„è²éŸ³çš„æƒ…å½¢ï¼Œtest index å‰‡ä»£è¡¨è‘—ç¸½å…±æœ‰å¹¾å€‹äººï¼Œåœ¨é€™å¼µåœ–çš„æƒ…æ³ä»£è¡¨è‘—æœ‰ 6 å€‹äººã€‚è‡³æ–¼é¡è‰²çš„éƒ¨åˆ†ï¼Œç´…è‰²ä»£è¡¨è‘—é€™å€‹è²éŸ³ç‰‡æ®µæ˜¯èª°çš„è²éŸ³ï¼Œè—è‰²çš„å‰‡ä»£è¡¨ä¸æ˜¯é€™å€‹äººçš„è²éŸ³ã€‚æ‰€ä»¥åƒæ˜¯é€™å¼µåœ–ç‰‡ä»£è¡¨è‘—æ¨¡å‹é æ¸¬æ­£ç¢ºï¼Œå› ç‚ºæ¨¡å‹æŠ•ç¥¨å‡ºä¾†çš„çµæœæœ€é«˜çš„æ˜¯ index 0ï¼Œè€Œç´…è‰²ï¼Œä¹Ÿå°±æ˜¯çœŸæ­£çš„è²éŸ³ä¸»äººä¹Ÿæ˜¯ index 0ï¼Œæ‰€ä»¥å°±ä»£è¡¨è‘—é€™å€‹åœ¨é€™æ¬¡çš„æ¸¬è©¦ä¸­ï¼Œæ¨¡å‹èƒ½å¤ æ­£ç¢ºåœ°é æ¸¬å‡ºçµæœã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%208.png)

# Results & Analysis & Others

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%209.png)

[10 äººå¯¦é©—æ•¸æ“š](https://www.notion.so/86fa688576634b52932da298104d3dc7?pvs=21)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2010.png)

[6 äººå¯¦é©—æ•¸æ“š](https://www.notion.so/0acfd10316c943c293dcff16275a7a14?pvs=21)

## Type of experiment

### Training steps

- Thoughts
    - å¯¦é©— 10-3 vs å¯¦é©— 10-4
    - å¤§æ–¼ 10000 å¯¦é©—æ­¥æ•¸å°æ–¼æœ€çµ‚çµæœçš„å½±éŸ¿ä¸å¤§
    - é¿å… overfitting
    - ç¯€çœé‹ç®—è³‡æº

Through the comparison of experiments 10-3 and 10-4, we observed that despite the significantly fewer training steps in experiment 10-3, the results were comparable to experiment 10-4. This finding suggests that reducing the number of training steps does not have a significant impact on the final performance of the model. This not only helps save computational resources but also mitigates the risk of overfitting. Therefore, we have set the number of training steps to 10,000 for subsequent experiments.

It is crucial to strike a balance between model training and the data available to avoid both overfitting and underfitting. By maintaining this balance, we can mitigate the risk of overfitting while ensuring that the model learns effectively from the data. This approach not only ensures the integrity of our model's performance but also enables us to allocate computational resources more efficiently.

é€éæ¯”è¼ƒå¯¦é©—10-3å’Œå¯¦é©—10-4ï¼Œé›–ç„¶å¯¦é©—10-3çš„è¨“ç·´æ­¥æ•¸æ¯”å¯¦é©—10-4å°‘äº†å¾ˆå¤šï¼Œä½†æ˜¯æˆ‘å€‘å¯ä»¥è§€å¯Ÿåˆ°å¯¦é©—10-3çš„çµæœå’Œå¯¦é©—10-4çš„çµæœå·®ä¸å¤šï¼Œä¸¦ä¸æœƒå°æ¨¡å‹çš„æœ€çµ‚çµæœæœ‰é¡¯è‘—çš„å½±éŸ¿ã€‚é€™å€‹ç™¼ç¾é™¤äº†å¯ä»¥æ›¿æˆ‘å€‘ç¯€çœé‹ç®—è³‡æºï¼Œä¹Ÿå¯ä»¥é¿å…æ¨¡å‹ç”¢ç”Ÿéæ“¬åˆã€‚æ‰€ä»¥æˆ‘å€‘ä¹‹å¾Œçš„è¨“ç·´æ­¥æ•¸éƒ½æ˜¯è¨­åœ¨10000æ­¥ã€‚åœ¨æ¨¡å‹çš„è¨“ç·´å’Œè³‡æ–™é›†çš„æ•¸æ“šä¹‹é–“ä¿æŒå¹³è¡¡ï¼Œä¸è¦è®“ä»–éæ“¬åˆä¹Ÿä¸è¦è®“ä»–å®Œå…¨å­¸ä¸åˆ°æ±è¥¿ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å¯ä»¥æ¸›è¼•éåº¦æ“¬åˆçš„é¢¨éšªï¼Œä¸¦ä¸”èƒ½å¤ æ›´å¥½çš„åˆ©ç”¨è³‡æºã€‚é€™ç¨®æ–¹æ³•ä¸åƒ…ç¢ºä¿äº†æˆ‘å€‘æ¨¡å‹æˆæœçš„å®Œæ•´æ€§ï¼Œé‚„ç¢ºä¿æˆ‘å€‘èƒ½å¤ æœ‰æ•ˆåˆ†é…è¨ˆç®—è³‡æºã€‚

### With RNN vs Without RNN

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2011.png)

[Untitled Database](https://www.notion.so/0969dd7f728640a39dcee38fd22f9176?pvs=21)

- Thoughts
    - å¯¦é©— 6-3 vs å¯¦é©— 6-4 å’Œ å¯¦é©— 6-9
    - å¯¦é©— 6-5 vs å¯¦é©— 6-2
    - éƒ½æ²’æœ‰ RNN
    - ä¸ç®¡è·Ÿ 1 layer æˆ– 2 layer RNN æ¯”æº–ç¢ºç‡éƒ½æ¥µä½ï¼Œå› æ­¤å¾ŒçºŒé¸æ“‡ RNN é€²è¡Œå¯¦ä½œã€‚

Based on the comparison between experiments that include and exclude the use of RNN, we can observe that models incorporating RNN tend to yield better results compared to models without RNN. This observation is evident in the 6-person experiments, where experiments 6-3 and 6-5 do not utilize RNN, while the rest of the experiments involving 6 individuals employ RNN. Similarly, in the 10-person experiments, experiments 10-6 and 10-8 do not use RNN, while the others do.

The results presented in the table indicate that the inclusion of RNN in the model architecture has a positive impact on performance. This finding suggests that exploring other methods or architectures that involve RNN could potentially further enhance the performance of our model. Therefore, in the upcoming stages, our focus will primarily be on training models that incorporate RNN.

æ¥ä¸‹ä¾†æ˜¯æ¯”è¼ƒæœ‰æ²’æœ‰ä½¿ç”¨ RNN å°çµæœå¥½å£å½±éŸ¿çš„å·®ç•°ã€‚åƒæ˜¯åœ¨å…­äººçš„å¯¦é©—ä¸­ï¼Œå¯¦é©— 6-3 å’Œ 6-5 æ²’æœ‰ä½¿ç”¨åˆ° RNNï¼Œé™¤æ­¤ä¹‹å¤–6äººçš„å¯¦é©—ä¸­éƒ½æœ‰ä½¿ç”¨åˆ° RNNã€‚è€Œ10äººçš„éƒ¨åˆ†ï¼Œå¯¦é©—10-6ä»¥åŠ10-8æ²’æœ‰ä½¿ç”¨åˆ°RNNï¼Œå…¶ä»–å‰‡éƒ½æœ‰ä½¿ç”¨åˆ°ã€‚å¾è¡¨æ ¼ä¾†çœ‹ï¼Œæˆ‘å€‘å¯ä»¥ç™¼ç¾æœ‰ä½¿ç”¨ RNN çš„æ¨¡å‹æ•ˆæœæœƒæ¯”æ²’æœ‰ä½¿ç”¨ RNN çš„æ¨¡å‹æ•ˆæœé‚„è¦å¥½ï¼Œé€™å€‹è§€å¯Ÿçµæœä»£è¡¨æˆ‘å€‘éœ€è¦å»å°‹æ‰¾å…¶ä»–æœ‰ä½¿ç”¨åˆ°RNNçš„æ–¹æ³•æˆ–æ¶æ§‹ä¾†è®“æˆ‘å€‘çš„æ¨¡å‹è¡¨ç¾æ›´å¥½ã€‚å› æ­¤ï¼Œåœ¨æ¥ä¸‹ä¾†çš„éƒ¨åˆ†ï¼Œæˆ‘å€‘å°‡ä¸»è¦ä½¿ç”¨æœ‰ RNN çš„æ¨¡å‹é€²è¡Œè¨“ç·´ã€‚

### 1 layer RNN vs 2 layer RNN

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2012.png)

[Untitled Database](https://www.notion.so/9e75413aae7d46499caf8916720b2eea?pvs=21)

- Thoughts
    - LSTM
        - Conv1d, 0 real: [å¯¦é©— 6-2](https://www.notion.so/6-2-95aa5b85a4094b9fbc19d8b025dfde63?pvs=21) vs [å¯¦é©— 6-12](https://www.notion.so/6-12-8b2512211889406cbf4252db6c2123db?pvs=21) (5 vs 5)
        - Conv2d, 2 real: [å¯¦é©— 6-8 (2 real)](https://www.notion.so/6-8-2-real-738cb998326c4f088c153e4b86413709?pvs=21) vs [å¯¦é©— 6-10 (2 real)](https://www.notion.so/6-10-2-real-c67ba9b853b3437481cccdd92a373316?pvs=21) (5 vs 5)
        - Conv2d, 0 real: [å¯¦é©— 6-4](https://www.notion.so/6-4-100e50f99eab4c009f2f9f5ba43cad55?pvs=21) vs [å¯¦é©— 6-9](https://www.notion.so/6-9-627d56926cde427cbef200ddcf0ca624?pvs=21) (6 vs 6)
    - GRU
        - Conv2d, 0 real: [å¯¦é©— 6-13](https://www.notion.so/6-13-d7a85b6c9a7e42539b592c7262ad67db?pvs=21) vs [å¯¦é©— 6-14](https://www.notion.so/6-14-65a5f94e5b5d48528f8c697e1d5ae033?pvs=21) (5 vs 5)
        - Conv1d, 0 real: [å¯¦é©— 6-11](https://www.notion.so/6-11-1cf9da77811a4d2a99257e40bcd7166e?pvs=21) vs [å¯¦é©— 6-15](https://www.notion.so/6-15-9926f41048544dd2bc1c378403bb5dfb?pvs=21) (5 vs 2)
    - LSTMçµ„å’ŒGRUçµ„çš„è¡¨ç¾å¤§è‡´ç›¸åŒã€‚
    - è¡¨ç¾å¤§è‡´ç›¸åŒï¼Œä½† 2 layer GRU åœ¨ Conv1d 0 real æ™‚è¡¨ç¾è¼ƒ 1 layer GRU å·®ã€‚æ ¹æ“šé€™ç¯‡[æ–‡ç« ](https://stackoverflow.com/questions/60585350/gru-model-overfits)ï¼Œæ¨æ¸¬å¯èƒ½æ˜¯å› ç‚ºå¤šå±¤çš„ GRU é€ æˆ overfitting çš„ç¾è±¡ï¼Œä¸” Conv1d é›£ä»¥æå–ç‰¹å¾µæ‰€å°è‡´ã€‚
    - 1 layer å¹³å‡è¨“ç·´é€Ÿåº¦æ›´å¿«ï¼Œä¸¦ä¸” 2 layer åœ¨æº–ç¢ºåº¦æ²’æœ‰å„ªå‹¢ï¼Œå› æ­¤ä¹‹å¾Œä¸»è¦é‡å° 1 layer é€²è¡Œè¨è«–ã€‚

During our experiments with different models and architectures, we observed some interesting phenomena. For LSTM, experiments 6-2 and 6-12, which used Conv1d without real human voice, showed similar performance, with an accuracy of correctly identifying 5 out of 6 individuals. Similarly, in the case of using Conv2d with two real human voices, experiments 6-8 and 6-10 had similar results, with an accuracy of correctly identifying 5 out of 6 individuals. On the other hand, when using Conv2d without real human voice, experiments 6-4 and 6-9 exhibited similar performance, with each experiment achieving an accuracy of correctly identifying all 6 individuals.

In the case of GRU models, experiments 6-13 and 6-14, which used Conv2d without real human voice, achieved an accuracy of correctly identifying 5 out of 6 individuals. Furthermore, experiments 6-11 and 6-15, which used Conv1d without real human voice, had accuracies of correctly identifying 5 and 2 individuals, respectively.

LSTM and GRU models performed similarly overall, but in the case of using Conv1d without real human voice, the performance of the 2-layer GRU model was not as good as the 1-layer GRU model. Based on [an article from Stack Overflow](https://stackoverflow.com/questions/60585350/gru-model-overfits), we speculate that this difference may be due to the tendency of multi-layer GRU models to overfit. Additionally, Conv1d might have difficulty extracting meaningful features, which could contribute to the lower performance of the 2-layer GRU model with Conv1d. Therefore, considering that the 1-layer model has faster training speed and the 2-layer model does not offer better accuracy, the subsequent discussion will primarily focus on the 1-layer model.

æˆ‘å€‘åœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å’Œæ¶æ§‹é€²è¡Œå¯¦é©—æ™‚ï¼Œç™¼ç¾äº†ä¸€äº›æœ‰è¶£çš„ç¾è±¡ã€‚åƒæ˜¯å°± LSTM è€Œè¨€ï¼Œä½¿ç”¨ Conv1d è€Œä¸”æ²’æœ‰çœŸäººè²éŸ³çš„å¯¦é©— 6-2 å’Œå¯¦é©— 6-12 çš„è¡¨ç¾ç›¸ä¼¼ï¼Œæ­£ç¢ºç‡éƒ½æ˜¯ 6 äººèƒ½æ­£ç¢ºè¾¨è­˜ 5äººã€‚åŒæ¨£åœ°ï¼Œå°æ–¼ä½¿ç”¨ Conv2d ä¸”æœ‰ä½¿ç”¨ 2 å€‹çœŸäººè²éŸ³çš„æƒ…æ³ï¼Œæœ‰ä½¿ç”¨2å€‹çœŸäººè²éŸ³å¯¦é©— 6-8å’Œå¯¦é©— 6-10çš„çµæœä¹Ÿç›¸è¿‘ï¼Œæ­£ç¢ºç‡éƒ½æ˜¯ 6 äººèƒ½æ­£ç¢ºè¾¨è­˜ 5äººã€‚å¦ä¸€æ–¹é¢ï¼Œç•¶ä½¿ç”¨ Conv2d ä¸”æ²’æœ‰çœŸå¯¦äººè²æ™‚ï¼Œå¯¦é©— 6-4 å’Œå¯¦é©— 6-9 éƒ½æœ‰ç›¸ä¼¼çš„è¡¨ç¾ï¼Œæ¯å€‹å¯¦é©—æ­£ç¢ºç‡éƒ½æ˜¯ 6 äººèƒ½æ­£ç¢ºè¾¨è­˜ 6äººã€‚
è‡³æ–¼åœ¨ GRU æ¨¡å‹ä¸­ï¼Œä½¿ç”¨ Conv2d ä¸”æ²’æœ‰çœŸäººè²éŸ³çš„å¯¦é©— 6-13 å’Œå¯¦é©— 6-14 æ­£ç¢ºç‡éƒ½æ˜¯ 6 äººèƒ½æ­£ç¢ºè¾¨è­˜ 5äººã€‚æ­¤å¤–ï¼Œä½¿ç”¨ Conv1d ä¸”æ²’æœ‰çœŸå¯¦äººè²çš„å¯¦é©— 6-11 å’Œå¯¦é©— 6-15 æ­£ç¢ºç‡åˆ†åˆ¥æ˜¯ 6 äººèƒ½æ­£ç¢ºè¾¨è­˜ 5äººå’Œ 2äººã€‚
LSTM å’Œ GRU å…©çµ„è¡¨ç¾å¤§è‡´ç›¸åŒï¼Œä½†æ˜¯åœ¨ä½¿ç”¨ Conv1d ä¸”æ²’æœ‰çœŸå¯¦äººè²æ™‚ï¼Œ2 å±¤ GRU æ¨¡å‹çš„è¡¨ç¾å»ä¸å¦‚ 1 å±¤ GRU æ¨¡å‹ã€‚æ ¹æ“šä¸€ç¯‡[ä¾†è‡ªStack Overflowçš„æ–‡ç« ](https://stackoverflow.com/questions/60585350/gru-model-overfits)ï¼Œæˆ‘å€‘çŒœæ¸¬é€™ç¨®å·®ç•°å¯èƒ½æ˜¯å› ç‚ºå¤šå±¤çš„ GRU æ¨¡å‹å®¹æ˜“éæ“¬åˆæ‰€å°è‡´çš„ã€‚æ­¤å¤–ï¼ŒConv1d å¯èƒ½ä¹Ÿæ¯”è¼ƒé›£æå–æœ‰æ„ç¾©çš„ç‰¹å¾µï¼Œå› æ­¤é€™å…©é»å°±å°è‡´äº†2 layer GRUä½¿ç”¨Conv1dæœƒæœ‰è¼ƒå·®çš„è¡¨ç¾ã€‚æ‰€ä»¥è€ƒæ…®åˆ°1å±¤çš„æ¨¡å‹è¨“ç·´é€Ÿåº¦è¼ƒå¿«ä¸” 2 å±¤æ¨¡å‹ä¸¦æ²’æœ‰è¼ƒå¥½çš„æº–ç¢ºåº¦ï¼Œå¾ŒçºŒçš„è¨è«–å°‡ä¸»è¦é›†ä¸­åœ¨1å±¤æ¨¡å‹ä¸Šã€‚

### LSTM vs GRU

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2013.png)

[Untitled Database](https://www.notion.so/13fe64ed67f84c2cadc10e10b34a28bd?pvs=21)

- Thoughts
    - 10äºº LSTM vs GRU
        - Conv1d, 1-layer: [å¯¦é©— 10-2](https://www.notion.so/10-2-1cf57c1739b24334ab9dfbd1ef53d0d0?pvs=21) vs [å¯¦é©— 10-1](https://www.notion.so/10-1-63703ede4ebc43928b9491404cad408a?pvs=21) (7 vs 6)
        - Conv2d, 1-layer: [å¯¦é©— 10-5](https://www.notion.so/10-5-3132f207e20f4c5481120fe52654b2c7?pvs=21) vs [å¯¦é©— 10-9](https://www.notion.so/10-9-9e2f118284ba431abf45c7bcea0f5f57?pvs=21) (5 vs 7)
        - Conv1d, 2-layer: [å¯¦é©— 10-3](https://www.notion.so/10-3-6a3159414cd74f54b4c9b3fbad9c6d1e?pvs=21) vs [å¯¦é©— 10-7](https://www.notion.so/10-7-1ad1db67f13b4e6a9589688d3b66932a?pvs=21) (7 vs 3)
        - Conv2d, 2-layer: [å¯¦é©— 10-11](https://www.notion.so/10-11-a6303ad79b4d4b5593533d7acf4ae225?pvs=21) vs [å¯¦é©— 10-10](https://www.notion.so/10-10-90e8d033c97b42ea9a513aea7d3c973f?pvs=21) (7 vs 6)
    - 6äºº LSTM vs GRU
        - Conv1d, 1-layer, 0 real: [å¯¦é©— 6-2](https://www.notion.so/6-2-95aa5b85a4094b9fbc19d8b025dfde63?pvs=21) vs [å¯¦é©— 6-11](https://www.notion.so/6-11-1cf9da77811a4d2a99257e40bcd7166e?pvs=21) (5 vs 5)
        - Conv2d, 1-layer, 0 real: [å¯¦é©— 6-4](https://www.notion.so/6-4-100e50f99eab4c009f2f9f5ba43cad55?pvs=21)  vs [å¯¦é©— 6-13](https://www.notion.so/6-13-d7a85b6c9a7e42539b592c7262ad67db?pvs=21) (6 vs 5)
        - Conv1d, 2-layer, 0 real: [å¯¦é©— 6-12](https://www.notion.so/6-12-8b2512211889406cbf4252db6c2123db?pvs=21) vs [å¯¦é©— 6-15](https://www.notion.so/6-15-9926f41048544dd2bc1c378403bb5dfb?pvs=21) (5 vs 2)
        - Conv2d, 2-layer, 0 real: [å¯¦é©— 6-9](https://www.notion.so/6-9-627d56926cde427cbef200ddcf0ca624?pvs=21) vs [å¯¦é©— 6-14](https://www.notion.so/6-14-65a5f94e5b5d48528f8c697e1d5ae033?pvs=21) (6 vs 5)
        - Conv1d, 1-layer, 1 real: [å¯¦é©— 6-6 (1 real)](https://www.notion.so/6-6-1-real-f43ccbb96421484384579d57e86542e8?pvs=21) vs [å¯¦é©— 6-18 (1 real)](https://www.notion.so/6-18-1-real-41442b2945684adf9bacf213891727a9?pvs=21) (5 vs 6)
        - Conv2d, 1-layer, 1 real: [å¯¦é©— 6-20 (1 real)](https://www.notion.so/6-20-1-real-fd863471ae5a4ae1bce80c628cbe8bd4?pvs=21) vs [å¯¦é©— 6-19 (1 real)](https://www.notion.so/6-19-1-real-812c4af3c52a4ba7ae3c3fe219ddcc49?pvs=21) (5 vs 5)
        - Conv1d, 1-layer, 2 real: [å¯¦é©— 6-7 (2 real)](https://www.notion.so/6-7-2-real-bfe1b62d8353429e8f46926dc699b867?pvs=21) vs [å¯¦é©— 6-17 (2 real)](https://www.notion.so/6-17-2-real-5cf36bbef5ff432c931b4a1e8de79b72?pvs=21) (3 vs 1)
        - Conv2d, 1-layer, 2 real: [å¯¦é©— 6-8 (2 real)](https://www.notion.so/6-8-2-real-738cb998326c4f088c153e4b86413709?pvs=21) vs [å¯¦é©— 6-16 (2 real)](https://www.notion.so/6-16-2-real-70b9c688b3774d13a7f4adc715e368f1?pvs=21) (5 vs 5)
    - GRU æ”¶æ–‚è¼ƒ LSTM å¿«ã€‚
    - æº–ç¢ºåº¦å¤§å¤šæƒ…æ³ä¸‹ GRU å°æ–¼ç­‰æ–¼ LSTMã€‚
    - 6äººçµ„ 2 realä¸­ï¼ŒConv1d LSTM é¡¯è‘—å„ªæ–¼ Conv1d GRUï¼Œ2 real å‰‡å¤§è‡´ç›¸åŒã€‚
    - 10äººçµ„ 1-layer Conv2d GRU å„ªæ–¼ LSTMï¼Œæ¨æ¸¬æ˜¯å› ç‚º GRU å¿«é€Ÿæ”¶æ–‚çš„ç‰¹æ€§ä½¿å…¶æ›´é©åˆå¤šäººæ•¸æ™‚çš„è¨“ç·´ã€‚

We can observe that the GRU model converges faster than the LSTM model when there are 10 individuals. Additionally, in terms of accuracy, GRU generally performs similarly to LSTM or slightly worse. However, in the case of a 1-layer Conv2d, the experiment with a GRU of 10-9 performs better than the LSTM result of 10-5. We speculate that this could be due to unlucky training circumstances.

In the scenario of 6 individuals without real human voice, both GRU and LSTM achieve similar results, except for the 2-layer Conv1d GRU, which performs significantly worse than LSTM. The accuracies are 2 individuals correctly identified out of 6 and 5 individuals correctly identified out of 6, respectively. We speculate that this could be due to unforeseen circumstances, such as particularly poor training luck, or the training steps for the 2-layer GRU should not have been set to 10,000 steps. Its fast convergence may have led to overfitting, resulting in poorer final results.

Lastly, in the case of 6 individuals with real human voice, when there is only 1 real human voice, LSTM and GRU achieve similar results. However, when there are 2 real human voices, it can be observed that using Conv2d with GRU and LSTM performs significantly better than using Conv1d with GRU and LSTM. We speculate that this could be because human voices are more unpredictable than Bark, and therefore, using Conv2d for transformation allows the models to capture features better and achieve more accurate recognition.

æˆ‘å€‘å¯ä»¥è§€å¯Ÿåˆ°åœ¨ 10 äººçš„æ™‚å€™ï¼Œ GRU æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦æ¯” LSTM æ¨¡å‹å¿«ã€‚å¦å¤–å°±æº–ç¢ºåº¦è€Œè¨€ï¼ŒGRU çš„è¡¨ç¾é€šå¸¸å’Œ LSTM ç›¸åŒæˆ–æ˜¯ç¨å¾®å·®ä¸€é»ã€‚ä¸éåœ¨ 1 layer Conv2d çš„æ™‚å€™ï¼Œå¯¦é©— 10-9 çš„ GRU å»èƒ½å¤ æ¯”å¯¦é©— 10-5 çš„ LSTM çµæœé‚„è¦å¥½ï¼Œæˆ‘å€‘çŒœæ¸¬æœ‰å¯èƒ½æ˜¯å› ç‚ºè¨“ç·´çš„é‹æ°£ä¸å¥½ã€‚

è‡³æ–¼åœ¨ 6 äººè€Œä¸”æ²’æœ‰çœŸäººè²éŸ³çš„æƒ…æ³ä¸‹ï¼ŒGRUä»¥åŠLSTMçš„æˆæœå¤§è‡´ç›¸ç­‰ï¼Œä½†æ˜¯å¯¦é©—6-15æ¯”å¯¦é©—6-12çš„æˆæœé‚„è¦å·®éå¸¸å¤šï¼Œåˆ†åˆ¥æ˜¯6å€‹äººæœ‰æ­£ç¢ºè¾¨è­˜åˆ°2äººä»¥åŠ6å€‹äººæœ‰æ­£ç¢ºè¾¨è­˜åˆ°5äººã€‚æˆ‘å€‘çŒœæ¸¬æœ‰å¯èƒ½æ˜¯å› ç‚ºæ„å¤–ç‹€æ³ï¼Œåƒæ˜¯è¨“ç·´çš„é‹æ°£ç‰¹åˆ¥å·®ï¼Œæˆ–æ˜¯2 layer GRUè¨­ç½®çš„è¨“ç·´æ­¥æ•¸ä¸è©²è¨­ç½®æˆ10000æ­¥ï¼Œå› ç‚ºä»–æœƒå¿«é€Ÿæ”¶æ–‚ï¼Œå¯èƒ½å¤ªå¿«å°±å°è‡´éæ“¬åˆçš„æƒ…å½¢ï¼Œå°è‡´æœ€çµ‚çµæœè¼ƒå·®ã€‚

æœ€å¾Œï¼Œåœ¨6äººä¸”æœ‰çœŸäººè²éŸ³çš„æƒ…æ³ä¸‹ï¼Œåœ¨åªæœ‰1å€‹çœŸäººè²éŸ³çš„æ™‚å€™LSTMè·ŸGRUçš„æˆæœä¹Ÿéƒ½å¤§è‡´ç›¸ç­‰ï¼Œä½†æ˜¯åœ¨2å€‹çœŸäººè²éŸ³çš„æƒ…æ³æ™‚ï¼Œå¯ä»¥ç™¼ç¾ä½¿ç”¨Conv2dçš„GRUä»¥åŠLSTMèƒ½å¤ æ¯”Conv1dçš„GRUä»¥åŠLSTMé‚„è¦å¥½å¾ˆå¤šï¼Œæˆ‘å€‘çŒœæ¸¬é€™å€‹åŸå› æ˜¯å› ç‚ºéœ‡å¹…æ˜¯ä¸€å€‹æ•¸å­—ï¼Œæ‰€ä»¥ä»–æœƒå°æ•¸é‡ç´šå¾ˆæ•æ„Ÿï¼Œä½†æ˜¯Mel Spectrogram å»ä¸æœƒä¸æœƒå› ç‚ºæ•¸é‡ç´šè€Œå—åˆ°å¤§å¹…è®Šå‹•ï¼Œå› æ­¤è¦ä½¿ç”¨Conv2dé€²è¡Œè½‰æ›ï¼Œæ¨¡å‹æ‰èƒ½å¤ æ›´å¥½çš„æŠ“åˆ°ç‰¹å¾µï¼Œé€²è€Œæ­£ç¢ºçš„è¾¨è­˜ã€‚æ‰€ä»¥æ¥ä¸‹ä¾†æˆ‘æœƒé–‹å§‹åˆ†æåœ¨ä½¿ç”¨Conv1dä»¥åŠConv2då°æ–¼æ¨¡å‹è¡¨ç¾çš„å½±éŸ¿ã€‚

### Conv1d with original voice vs Conv2d with Mel Spectrogram

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2014.png)

[Untitled Database](https://www.notion.so/daaaa608f1c34881b9d646fcb43592ad?pvs=21)

- Thoughts
    - 10äºº
        - LSTM, 1 layer: [å¯¦é©— 10-2](https://www.notion.so/10-2-1cf57c1739b24334ab9dfbd1ef53d0d0?pvs=21) vs [å¯¦é©— 10-5](https://www.notion.so/10-5-3132f207e20f4c5481120fe52654b2c7?pvs=21) (7 vs 5)
        - GRU, 1 layer: [å¯¦é©— 10-1](https://www.notion.so/10-1-63703ede4ebc43928b9491404cad408a?pvs=21) vs [å¯¦é©— 10-9](https://www.notion.so/10-9-9e2f118284ba431abf45c7bcea0f5f57?pvs=21) (6 vs 7)
        - LSTM, 2 layer: [å¯¦é©— 10-3](https://www.notion.so/10-3-6a3159414cd74f54b4c9b3fbad9c6d1e?pvs=21) vs [å¯¦é©— 10-11](https://www.notion.so/10-11-a6303ad79b4d4b5593533d7acf4ae225?pvs=21) (7 vs 7)
        - GRU, 2 layer: [å¯¦é©— 10-7](https://www.notion.so/10-7-1ad1db67f13b4e6a9589688d3b66932a?pvs=21) vs [å¯¦é©— 10-10](https://www.notion.so/10-10-90e8d033c97b42ea9a513aea7d3c973f?pvs=21) (3 vs 6)
        - None: [å¯¦é©— 10-6](https://www.notion.so/10-6-f205eeeebdd248d69de5724c855ebade?pvs=21) vs [å¯¦é©— 10-8](https://www.notion.so/10-8-3907755272274532bf989e98a1f25ff5?pvs=21) (1 vs 0)
    - 6äºº
        - LSTM, 0 real, 1 layer: [å¯¦é©— 6-2](https://www.notion.so/6-2-95aa5b85a4094b9fbc19d8b025dfde63?pvs=21) vs [å¯¦é©— 6-4](https://www.notion.so/6-4-100e50f99eab4c009f2f9f5ba43cad55?pvs=21) (5 vs 6)
        - GRU, 0 real, 1 layer: [å¯¦é©— 6-11](https://www.notion.so/6-11-1cf9da77811a4d2a99257e40bcd7166e?pvs=21) vs [å¯¦é©— 6-13](https://www.notion.so/6-13-d7a85b6c9a7e42539b592c7262ad67db?pvs=21) (5 vs 5)
        - LSTM, 0 real, 2 layer: [å¯¦é©— 6-12](https://www.notion.so/6-12-8b2512211889406cbf4252db6c2123db?pvs=21) vs [å¯¦é©— 6-9](https://www.notion.so/6-9-627d56926cde427cbef200ddcf0ca624?pvs=21) (5 vs 6)
        - GRU, 0 real, 2 layer: [å¯¦é©— 6-15](https://www.notion.so/6-15-9926f41048544dd2bc1c378403bb5dfb?pvs=21) vs [å¯¦é©— 6-14](https://www.notion.so/6-14-65a5f94e5b5d48528f8c697e1d5ae033?pvs=21) (2 vs 5)
        - LSTM, 1 real, 1 layer: [å¯¦é©— 6-6 (1 real)](https://www.notion.so/6-6-1-real-f43ccbb96421484384579d57e86542e8?pvs=21) vs [å¯¦é©— 6-20 (1 real)](https://www.notion.so/6-20-1-real-fd863471ae5a4ae1bce80c628cbe8bd4?pvs=21) (5 vs 5)
        - GRU, 1 real, 1 layer: [å¯¦é©— 6-18 (1 real)](https://www.notion.so/6-18-1-real-41442b2945684adf9bacf213891727a9?pvs=21) vs [å¯¦é©— 6-19 (1 real)](https://www.notion.so/6-19-1-real-812c4af3c52a4ba7ae3c3fe219ddcc49?pvs=21) (6 vs 5)
        - LSTM, 2 real, 1 layer: [å¯¦é©— 6-7 (2 real)](https://www.notion.so/6-7-2-real-bfe1b62d8353429e8f46926dc699b867?pvs=21) vs [å¯¦é©— 6-8 (2 real)](https://www.notion.so/6-8-2-real-738cb998326c4f088c153e4b86413709?pvs=21) (3 vs 5)
        - GRU, 2 real, 1 layer: [å¯¦é©— 6-17 (2 real)](https://www.notion.so/6-17-2-real-5cf36bbef5ff432c931b4a1e8de79b72?pvs=21) vs [å¯¦é©— 6-16 (2 real)](https://www.notion.so/6-16-2-real-70b9c688b3774d13a7f4adc715e368f1?pvs=21) (1 vs 5)
    - å¯¦é©— 6-2 vs å¯¦é©— 6-4
        - 6 çµ„æ­£è¦åŒ–çš„è²éŸ³ï¼ŒConv1d ç•¥éœæ–¼ Conv2d
        - æ¨æ¸¬æ˜¯å› ç‚ºäººæ•¸è¼ƒå°‘ï¼ŒConv2d çš„å„ªå‹¢é–‹å§‹é¡¯ç¾
    - å¯¦é©— 10-2 vs å¯¦é©— 10-5
        - 10 çµ„æ­£è¦åŒ–çš„è²éŸ³ï¼ŒConv1d å‹é Conv2d
        - æ¨æ¸¬æ˜¯å› ç‚ºäººæ•¸è¼ƒå¤šï¼Œé›™æ–¹çš„æå¤±éƒ½åœ¨å¾ˆé«˜çš„æ•¸å€¼ï¼ˆ2.22ï¼‰
        - æ¨¡å‹è™•æ–¼æ··äº‚ç‹€æ…‹ï¼Œç„¡æ³•ç¢ºå®šä½•è€…è¼ƒä½³
    - å¯¦é©— 6-7 vs å¯¦é©— 6-8
        - 6 çµ„æœªç¶“æ­£è¦åŒ–çš„è²éŸ³ï¼ŒConv1d æ•—æ–¼ Conv2d
        - æ¨æ¸¬æ˜¯å› ç‚ºæ•¸é‡ç´šçš„å½±éŸ¿å°æ–¼ Mel Spectrogram è€Œè¨€ä¸é¡¯è‘—ï¼Œä½†æ˜¯å°æ–¼æŒ¯å¹…ä¾†èªªç›¸ç•¶é‡è¦
    - 10äººçµ„ä¸­çš„ 2 layer GRU å’Œ None å› ç‚ºå‰è¿°å¯¦é©—ä¸­çµ¦å‡ºå…·æœ‰ç¼ºé™·çš„çµè«–ï¼Œæœ¬è™•ä¸äºˆè€ƒæ…®ã€‚
    - 10äººçµ„ä¸­ Conv1d è¡¨ç¾ç•¥å‹ Conv2dã€‚
    - 6äººçµ„é™¤å» 1 layer GRUï¼ŒConv2d æº–ç¢ºåº¦çš†å„ªæ–¼ Conv1dï¼Œæ¨æ¸¬æ˜¯å› ç‚º Mel Spectrogram èƒ½æ›´å¥½çš„æå–è²éŸ³è³‡æ–™ä¹‹ç‰¹å¾µã€‚
    - åœ¨ 2 real 1 layer LSTMï¼Œå³æœ¬è¨ˆç•«è¼ƒå…·å¯¦ç”¨åƒ¹å€¼ä¹‹æˆæœä¸­ï¼ŒMel Spectrogram é¡¯è‘—å„ªæ–¼ Conv1dï¼Œæ•…å»ºè­°ç›¡é‡ä½¿ç”¨ Mel Spectrogram ä¾†é€²è¡Œè¨“ç·´ã€‚

We can observe that, with the exception of experiments 10-6 and 10-8, which performed poorly due to the lack of RNN usage, the remaining experiments show decent performance when there are 10 individuals. However, in the case of a 2-layer GRU, we notice that experiment 10-7 performs significantly worse than experiment 10-10, correctly identifying only 3 individuals compared to 6 individuals. This discrepancy could be attributed to the potential overfitting issue caused by Conv1d.

In the scenario of 6 individuals without real human voices, the performance is generally comparable across the experiments. However, in the case of a 2-layer GRU, experiment 6-15 performs much worse than experiment 6-14, correctly identifying only 2 individuals instead of 5. This could also be attributed to the aforementioned issue of overfitting in the 2-layer GRU.

Lastly, in the scenario of 6 individuals with real human voices, we observe that the results of Conv2d are consistently accurate, correctly identifying 5 individuals. On the other hand, the results of Conv1d are highly inconsistent, ranging from 1 to 6 individuals correctly identified. This further confirms the earlier mentioned observation that using Conv2d for feature extraction enables the model to better capture features and consequently make accurate identifications.

æˆ‘å€‘å¯ä»¥è§€å¯Ÿåˆ°åœ¨ 10 äººçš„æ™‚å€™ï¼Œ é™¤äº†å¯¦é©—10-6ä»¥åŠå¯¦é©—10-8æ˜¯å› ç‚ºæ²’æœ‰ä½¿ç”¨RNNæ‰€ä»¥å°è‡´çµæœå¾ˆå·®ä»¥å¤–ï¼Œå…¶ä»–éƒ½èƒ½å¤ æœ‰ä¸éŒ¯çš„è¡¨ç¾ã€‚ä½†æ˜¯åœ¨2 layer GRUä¸­ï¼Œå¯ä»¥ç™¼ç¾å¯¦é©—10-7æ¯”å¯¦é©—10-10çš„çµæœé‚„è¦å·®å¾ˆå¤šï¼Œåˆ†åˆ¥æ˜¯10å€‹è£¡é¢æ­£ç¢ºè¾¨è­˜3äººä»¥åŠ6äººï¼Œæˆ‘å€‘èªç‚ºå¯èƒ½æ˜¯Conv1dæœƒå¤ªå®¹æ˜“overfittingæ‰€å°è‡´çš„ã€‚

è‡³æ–¼åœ¨ 6 äººè€Œä¸”æ²’æœ‰çœŸäººè²éŸ³çš„æƒ…æ³ä¸‹ï¼Œè¡¨ç¾çš†å¤§è‡´ç›¸ç­‰ï¼Œä½†æ˜¯åœ¨ 2 layer GRUçš„æƒ…æ³åº•ä¸‹ï¼Œå¯¦é©—6-15å»æ¯”å¯¦é©—6-14é‚„è¦å·®å¾ˆå¤šï¼Œåˆ†åˆ¥æ˜¯æ­£ç¢ºè¾¨è­˜2äººä»¥åŠ5äººã€‚å¯èƒ½å°±æ˜¯å› ç‚ºå‰é¢æåˆ°çš„ï¼Œ2 layer GRUæ›´å®¹æ˜“overfittingæ‰€å°è‡´çš„ã€‚

æœ€å¾Œï¼Œåœ¨6äººä¸”æœ‰çœŸäººè²éŸ³çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥ç™¼ç¾Conv2dçš„çµæœå¤§è‡´ä¸Šéƒ½éå¸¸ç©©å®šçš„è½åœ¨æ­£ç¢ºè¾¨è­˜5äººï¼Œä½†æ˜¯Conv1dçš„çµæœå»éå¸¸ä¸ç©©å®šï¼Œå¾1åˆ°6éƒ½æœ‰ã€‚é€™ä¹Ÿé©—è­‰äº†æˆ‘å‰›å‰›æ‰€æåˆ°çš„ï¼Œè¦ä½¿ç”¨Conv2dé€²è¡Œè½‰æ›ï¼Œæ¨¡å‹æ‰èƒ½å¤ æ›´å¥½çš„æŠ“åˆ°ç‰¹å¾µï¼Œé€²è€Œæ­£ç¢ºçš„è¾¨è­˜ã€‚

## Best result for future practical usages

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2015.png)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2016.png)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2017.png)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2018.png)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2019.png)

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2020.png)

- å¯¦é©— 6-4
    - Conv2d
    - LSTM layer=1
    - step=10000
    - learning rate=0.00003
    - weight decay=0.00001

The best experimental results we achieved on the dataset containing human voice were obtained using a model trained with Conv2d and a single layer LSTM, specifically the results of experiment 6-4. Additionally, in this model, we obtained consistent results with previous inferences.

Firstly, we used Conv2d instead of Conv1d due to the inclusion of human voice in the dataset. This choice was made to avoid the scaling issue caused by the different formats of audio files.

Secondly, we opted for a single layer LSTM instead of a two-layer LSTM because a single layer LSTM incurs lower computational costs, effectively saving training and inference time as well as computational resources. Furthermore, we chose a single layer LSTM over a single layer GRU because the GRU model tends to exhibit overfitting phenomena in training scenarios with 10,000 steps, a learning rate of 3e-5, and weight decay of 1e-5.

æˆ‘å€‘åœ¨åŒ…å«çœŸäººè²éŸ³çš„è³‡æ–™é›†ä¸Šï¼Œæ‰€å–å¾—çš„æœ€ä½³å¯¦é©—æˆæœæ˜¯åŸºæ–¼ Conv2d å’Œ 1 layer LSTM è¨“ç·´å‡ºçš„æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯å¯¦é©— 6-4 çš„çµæœã€‚å¦å¤–åœ¨æ­¤æ¨¡å‹ä¸­ï¼Œæˆ‘å€‘å¾—å‡ºäº†å’Œå…ˆå‰çš„æ¨è«–ä¸€è‡´çš„çµæœã€‚

é¦–å…ˆï¼Œç”±æ–¼è³‡æ–™é›†ä¸­åŒ…å«çœŸäººçš„è²éŸ³ï¼Œå› æ­¤ä½¿ç”¨ Conv2d è€Œé Conv1dï¼Œä»¥è¦é¿å› ç‚ºè²éŸ³æª”çš„æ ¼å¼ä¸åŒè€Œå°è‡´çš„æ•¸é‡ç´šå•é¡Œã€‚

å…¶æ¬¡ï¼Œä½¿ç”¨ 1 layer çš„ LSTM è€Œé 2 layer æ˜¯å› ç‚º 1 layer çš„ LSTM é‹ç®—æ‰€éœ€æˆæœ¬è¼ƒä½ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç¯€ç´„è¨“ç·´å’Œæ¨è«–çš„æ™‚é–“å’Œé‹ç®—è³‡æºã€‚æ­¤å¤–ï¼Œä¹‹æ‰€ä»¥ä½¿ç”¨ 1 layer çš„ LSTM è€Œé 1 layer çš„ GRUï¼Œå‰‡æ˜¯å› ç‚º GRU çš„æ¨¡å‹åœ¨ 10000 æ­¥ã€3e-5 çš„å­¸ç¿’ç‡å’Œ 1e-5 çš„æ¬Šé‡è¡°é€€çš„è¨“ç·´æƒ…å¢ƒä¸‹ï¼Œè¼ƒå®¹æ˜“å‡ºç¾éæ“¬åˆçš„ç¾è±¡ã€‚

## Limitation of our work

### 1. Limitation of training

We have limited computational resources and time constraints. Although we have access to a GPU workstation provided by our instructor, we encountered difficulties in successfully training models on it. As a result, we conducted training on our local machines. Consequently, training a more complex model architecture like the 2 layer LSTM requires more time, but the improvement in performance is not significant compared to the 1 layer LSTM. Therefore, we decided not to run the real human testing scenario with the 2 layer LSTM as it is not feasible within our limitations.

æˆ‘å€‘çš„è¨ˆç®—è³‡æºè·Ÿæ™‚é–“æœ‰é™ï¼Œé›–ç„¶æœ‰è€å¸«æä¾›çš„GPUå·¥ä½œç«™ï¼Œä½†æ˜¯æˆ‘å€‘ä¸çŸ¥é“ç‚ºä½•æ²’è¾¦æ³•æˆåŠŸåœ¨ä¸Šé¢é€²è¡Œæ¨¡å‹çš„è¨“ç·´ï¼Œå› æ­¤åœ¨è¨“ç·´çš„æ™‚å€™éƒ½æ˜¯åœ¨æˆ‘å€‘çš„æœ¬åœ°ç«¯é›»è…¦ã€‚æ‰€ä»¥åƒæ˜¯ 2 layer LSTM åœ¨æ¨¡å‹æ¶æ§‹ä¸Šæ¯” 1 layer é‚„è¦æ›´è¤‡é›œï¼Œéœ€è¦èŠ±è²»æ›´é•·çš„æ™‚é–“é€²è¡Œè¨“ç·´ï¼Œä½†æ˜¯æ•ˆæœå»æ²’æœ‰æ¯”1 layeré‚„è¦å¥½å¤šå°‘ï¼Œæ‰€ä»¥æˆ‘å€‘çœŸäººæ¸¬è©¦å°±æ²’æœ‰å»è·‘2 layer LSTM çš„æƒ…å½¢ï¼Œå› ç‚ºå¤ªä¸åˆ‡å¯¦éš›äº†ã€‚

### 2. Cannot specify when there are too many people

In the dataset, when testing with only two voices, the model achieves a high accuracy rate close to 100%. However, when there are six voices, the accuracy drops to 83.3%, and with ten voices, it further decreases to 70%. We suspect that this issue could be addressed by applying a model that can capture more distinctive audio features.

åœ¨è³‡æ–™é›†ä¸­å¦‚æœåªæœ‰æ¸¬è©¦å…©å€‹è²éŸ³çš„æ™‚å€™ï¼Œæ¨¡å‹çš„æº–ç¢ºç‡å¯ä»¥æ¥è¿‘ 100%ã€‚ä½†æ˜¯ç•¶æœ‰ 6 å€‹è²éŸ³çš„æ™‚å€™ï¼Œæº–ç¢ºç‡å»æœƒä¸‹é™åˆ° 83.3%ï¼Œç•¶æœ‰ 10 å€‹è²éŸ³æ™‚ï¼Œæº–ç¢ºç‡æ›´æœƒä¸‹é™åˆ° 70%ã€‚æˆ‘å€‘çŒœæ¸¬é€™å¯ä»¥é€éæ‡‰ç”¨ä¸€å€‹èƒ½å°æŠ“å‡ºæ›´å¤šè²éŸ³ç‰¹å¾µçš„æ¨¡å‹ä¾†è§£æ±ºé€™å€‹å•é¡Œã€‚

### 3. Limitation on file formats

One limitation of our model is that it requires the use of normalized WAV files. During the experiments with real human voices, we discovered that the audio sources we used were not normalized WAV files. Since the voices generated by Bark were already in a normalized format, the presence of non-normalized voices severely affected the performance of our model. To ensure accuracy, it was necessary to convert the real human voices into mono-channel format and with a sampling rate of 24000 Hz.

Although we found methods to normalize the WAV files of the real human voices, it appears that this did not completely resolve the issue. However, when using Conv2D, the PyTorch library automatically handles the conversion of Mel Spectrograms, resulting in acceptable results. Nonetheless, it is essential to address this problem properly, potentially by considering alternative data sources with a more suitable format to ensure accurate performance.

æˆ‘å€‘æ¨¡å‹çš„å…¶ä¸­ä¸€å€‹é™åˆ¶æ˜¯éœ€è¦ä½¿ç”¨ç¶“éæ­£è¦åŒ–éå¾Œçš„çš„ WAV æ–‡ä»¶ã€‚å› ç‚ºåœ¨é€²è¡ŒçœŸäººè²éŸ³çš„å¯¦é©—æ™‚ï¼Œæˆ‘å€‘ç™¼ç¾æˆ‘å€‘ä½¿ç”¨çš„è²éŸ³ä¾†æºæ˜¯ä¸€å€‹æ²’æœ‰è¢«æ­£è¦åŒ–éçš„ WAV æ–‡ä»¶ã€‚å› ç‚ºæˆ‘å€‘ç”¨ Bark ç”Ÿæˆçš„è²éŸ³å·²ç¶“æ˜¯æ­£è¦åŒ–çš„å½¢å¼ï¼Œæ‰€ä»¥æ²’æœ‰è¢«æ­£è¦åŒ–çš„è²éŸ³æœƒåš´é‡å½±éŸ¿æˆ‘å€‘æ¨¡å‹çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œç‚ºäº†ä¿è­‰æº–ç¢ºæ€§ï¼Œéœ€è¦å°‡çœŸäººçš„è²éŸ³è½‰æ›ç‚ºå–®è²é“å½¢å¼ï¼Œä¸¦ä¸”å…·æœ‰ 24000 Hz çš„æ¡æ¨£ç‡ã€‚é›–ç„¶æˆ‘å€‘æœ‰æ‰¾åˆ°æ–¹æ³•è®“çœŸäººè²éŸ³çš„WAVæ–‡ä»¶æ­£è¦åŒ–ï¼Œä½†æ˜¯ä¼¼ä¹ä¸èƒ½å®Œå…¨è§£æ±ºé€™å€‹å•é¡Œï¼Œä¸éåœ¨ä½¿ç”¨Conv2dæ™‚ï¼Œpytorchçš„å‡½å¼åº«æœƒåœ¨è½‰æ› Mel Spectrogram çš„éç¨‹ä¸­å¹«æˆ‘å€‘è§£æ±ºé€™å€‹å•é¡Œï¼Œæ‰€ä»¥æœ€å¾Œçš„çµæœä¸¦æ²’æœ‰å¤ªå·®ï¼Œä½†æ˜¯å¯¦éš›ä¸Šæ‡‰è©²éœ€è¦è§£æ±ºé€™é …å•é¡Œï¼Œå¯èƒ½æˆ‘å€‘çš„è³‡æ–™ä¾†æºéœ€è¦æ›´æ”¹ï¼Œæ‰¾åˆ°ä¸€å€‹æ›´é©åˆçš„æ ¼å¼ã€‚

## Approach to applying the model/method to practical use.

We trained our model using voice generated by Bark and tested its ability to recognize real human voices. We conducted experiments involving one real human paired with five voices generated by Bark, as well as two real humans paired with four voices generated by Bark. For these experiments, we only used models that performed well during training.

In the case of one real human, each model achieved good results. However, when there were two real humans involved, the models using Conv1D with 1 layer GRU and Conv1D with 1 layer LSTM did not perform as well.

æˆ‘å€‘é€éBarkæ‰€ç”Ÿæˆå‡ºä¾†çš„è²éŸ³è¨“ç·´ï¼Œä¸¦åˆ©ç”¨é€™å€‹è²éŸ³æ¨¡å‹è­˜åˆ¥çœŸäººè²éŸ³ã€‚æˆ‘å€‘åªæœ‰æ¸¬è©¦åŠ å…¥ä¸€å€‹çœŸäººé…ä¸Šäº”å€‹ç”±Barkç”Ÿå‡ºä¾†çš„å‡äººä»¥åŠå…©å€‹çœŸäººé…ä¸Šå››å€‹ç”±Barkç”Ÿå‡ºä¾†çš„å‡äººçš„å¯¦é©—ã€‚æˆ‘å€‘éƒ½åªæœ‰æ‹¿åœ¨è¨“ç·´çµæœé‚„ä¸éŒ¯çš„æ¨¡å‹ä¾†è·‘æœ‰çœŸäººçš„æƒ…å½¢ã€‚åœ¨åªæœ‰ä¸€å€‹çœŸäººçš„æƒ…æ³åº•ä¸‹ï¼Œæ¯å€‹æ¨¡å‹éƒ½èƒ½å¤ æœ‰ä¸éŒ¯çš„çµæœï¼Œä½†æ˜¯åœ¨å…©å€‹çœŸäººçš„æƒ…æ³åº•ä¸‹ï¼ŒConv1d with 1 layer GRU å’Œ Conv1d with 1 layer LSTM çš„çµæœå»æ²’æœ‰éå¸¸å¥½ã€‚

![Untitled](AI%20final%20project%2008ac1598716140968d5415cd97d7e878/Untitled%2021.png)

[Untitled Database](https://www.notion.so/92ad550f274f47acaef38c96a052ae59?pvs=21)

# Reference

[GitHub - KuiZenith/voice-recognition](https://github.com/KuiZenith/voice-recognition)

[GitHub - suno-ai/bark: ğŸ”Š Text-Prompted Generative Audio Model](https://github.com/suno-ai/bark)

[25. ç°¡ä»‹LSTM èˆ‡GRU](https://medium.com/programming-with-data/25-ç°¡ä»‹lstm-èˆ‡gru-3e0eaa100d29)

[Audio Deep Learning Made Simple (Part 2): Why Mel Spectrograms perform better](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505)

[Guardians of the Galaxy | Chris Pratt, Zoe Saldana & Vin Diesel | Talks at Google](https://www.youtube.com/watch?v=YfQJed8PSLU&ab_channel=TalksatGoogle)
