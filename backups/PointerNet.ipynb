{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from bark.bark import SAMPLE_RATE, generate_audio, preload_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_models(use_smaller_models=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(basepath: str, audio_array: np.ndarray, prompt: str, generation_data: list):\n",
    "  np.savez(f\"{basepath}.npz\", semantic_prompt=generation_data[0], coarse_prompt=generation_data[1], fine_prompt=generation_data[2])\n",
    "  with open(f\"{basepath}.txt\", \"w\", encoding=\"utf-8\") as fp: fp.write(prompt)\n",
    "  wavfile.write(f\"{basepath}.wav\", SAMPLE_RATE, audio_array)\n",
    "\n",
    "def load_history(filepath: str):\n",
    "  return np.load(filepath)\n",
    "\n",
    "def load_voice(filepath: str):\n",
    "  sample_rate, audio_array = wavfile.read(filepath)\n",
    "  return audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, inputs_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.num_layers = 4\n",
    "    self.inputs_dim = inputs_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.lstm = nn.LSTM(self.inputs_dim, self.hidden_dim, self.num_layers // 2, dropout=0.1, bidirectional=True, batch_first=True)\n",
    "    self.h0 = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "    self.c0 = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "  def init_hidden(self, inputs):\n",
    "    batch_size = inputs.size(0)\n",
    "    h0 = self.h0.reshape((1, 1, 1)).repeat(self.num_layers, batch_size, self.hidden_dim)\n",
    "    c0 = self.c0.reshape((1, 1, 1)).repeat(self.num_layers, batch_size, self.hidden_dim)\n",
    "    return h0, c0\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    outputs, hidden = self.lstm(inputs, hidden)\n",
    "    return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "  def __init__(self, inputs_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.inputs_dim = inputs_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.inputs_fc = nn.Linear(inputs_dim, hidden_dim)\n",
    "    self.context_fc = nn.Conv1d(inputs_dim, hidden_dim, 1, 1)\n",
    "    self.V = nn.Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "    self._inf = nn.Parameter(torch.FloatTensor([float(\"-inf\")]), requires_grad=False)\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    nn.init.uniform_(self.V, -1, 1)\n",
    "\n",
    "  def init_inf(self, mask_size):\n",
    "    self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n",
    "\n",
    "  def forward(self, inputs, context, mask):\n",
    "    i = self.inputs_fc(inputs).unsqueeze(2).expand(-1, -1, context.size(1))\n",
    "    context = context.permute(0, 2, 1)\n",
    "    c = self.context_fc(context)\n",
    "    V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "    attention = torch.bmm(V, self.tanh(i + c)).squeeze(1)\n",
    "    if len(attention[mask]) > 0: attention[mask] = self.inf[mask]\n",
    "    alpha = self.softmax(attention)\n",
    "    hidden_state = torch.bmm(c, alpha.unsqueeze(2)).squeeze(2)\n",
    "    return hidden_state, alpha\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, inputs_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.inputs_dim = inputs_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.input_to_hidden = nn.Linear(self.inputs_dim, 4 * self.hidden_dim)\n",
    "    self.hidden_to_hidden = nn.Linear(self.hidden_dim, 4 * self.hidden_dim)\n",
    "    self.hidden_out = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "    self.attention = Attention(self.hidden_dim, self.hidden_dim)\n",
    "    self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "    self.runner = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "  def forward(self, inputs, decoder_input, hidden, context):\n",
    "    batch_size = inputs.size(0)\n",
    "    inputs_len = inputs.size(1)\n",
    "    mask = self.mask.repeat(inputs_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "    self.attention.init_inf(mask.size())\n",
    "    runner = self.runner.repeat(inputs_len)\n",
    "    for i in range(inputs_len): runner.data[i] = i\n",
    "    runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "    outputs = []\n",
    "    pointers = []\n",
    "\n",
    "    def step(x, hidden):\n",
    "      h, c = hidden\n",
    "      gates = self.input_to_hidden(x) + self.hidden_to_hidden(h)\n",
    "      input, forget, cell, out = gates.chunk(4, 1)\n",
    "      input = F.sigmoid(input)\n",
    "      forget = F.sigmoid(forget)\n",
    "      cell = F.tanh(cell)\n",
    "      out = F.sigmoid(out)\n",
    "      c_t = (forget * c) + (input * cell)\n",
    "      h_t = out * F.tanh(c_t)\n",
    "      hidden_t, output = self.attention(h_t, context, torch.eq(mask, 0))\n",
    "      hidden_t = F.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "      return hidden_t, c_t, output\n",
    "\n",
    "    for _ in range(inputs_len):\n",
    "      h_t, c_t, outs = step(decoder_input, hidden)\n",
    "      hidden = (h_t, c_t)\n",
    "      masked_outs = outs * mask\n",
    "      max_probs, indices = masked_outs.max(1)\n",
    "      one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "      mask *= 1 - one_hot_pointers\n",
    "      embedding_mask = one_hot_pointers.unsqueeze(2).expand(-1, -1, self.inputs_dim).byte()\n",
    "      decoder_input = inputs[embedding_mask.data.bool()].view(batch_size, self.inputs_dim)\n",
    "      outputs.append(outs.unsqueeze(0))\n",
    "      pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "    outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "    pointers = torch.cat(pointers, 1)\n",
    "    return (outputs, pointers), hidden\n",
    "\n",
    "class PointerNet(nn.Module):\n",
    "  def __init__(self, inputs_dim):\n",
    "    super().__init__()\n",
    "    self.inputs_dim = inputs_dim\n",
    "    self.embedding_dim = 256\n",
    "    self.hidden_dim = 128\n",
    "    self.embedding = nn.Sequential(\n",
    "      nn.Conv1d(1, 16, 4),\n",
    "      nn.MaxPool1d(2),\n",
    "      nn.Conv1d(16, 64, 4),\n",
    "      nn.MaxPool1d(2),\n",
    "      nn.Conv1d(64, 256, 4),\n",
    "      nn.MaxPool1d(2),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(8997, 1),\n",
    "    )\n",
    "    self.encoder = Encoder(self.embedding_dim, self.hidden_dim // 2)\n",
    "    self.decoder = Decoder(self.embedding_dim, self.hidden_dim)\n",
    "    self.decoder_input0 = nn.Parameter(torch.FloatTensor(self.embedding_dim), requires_grad=False)\n",
    "    nn.init.uniform_(self.decoder_input0, -1, 1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batch_size = inputs.size(0)\n",
    "    inputs_len = inputs.size(1)\n",
    "\n",
    "    decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "    inputs = inputs.view(batch_size * inputs_len, 1, -1)\n",
    "    embedded_inputs = torch.cat([self.embedding(inp) for inp in inputs]).view(batch_size, inputs_len, -1)\n",
    "\n",
    "    encoder_hidden0 = self.encoder.init_hidden(embedded_inputs)\n",
    "    encoder_outputs, encoder_hidden = self.encoder(embedded_inputs, encoder_hidden0)\n",
    "    decoder_hidden0 = (torch.cat(tuple(encoder_hidden[0][-2:]), dim=-1), torch.cat(tuple(encoder_hidden[1][-2:]), dim=-1))\n",
    "    (outputs, pointers), decoder_hidden = self.decoder(embedded_inputs, decoder_input0, decoder_hidden0, encoder_outputs)\n",
    "    return outputs, pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "MAX_DURATION = 3000\n",
    "DURATION = 3000\n",
    "SHIFT = 10\n",
    "\n",
    "EMBEDDING_DIM = int(DURATION * SAMPLE_RATE / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_voice(audio_array, period, max_period):\n",
    "  return [audio_array[start:start + period].tolist() for start in range(0, max_period, period)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1000:\t11.5129\n",
      "200/1000:\t11.5129\n",
      "300/1000:\t11.5129\n",
      "400/1000:\t11.5129\n",
      "500/1000:\t11.5129\n",
      "600/1000:\t11.5129\n",
      "700/1000:\t11.5129\n",
      "800/1000:\t11.5129\n",
      "900/1000:\t11.5129\n",
      "1000/1000:\t11.5129\n",
      "Training Ended!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "originals = {}\n",
    "voice_map = {}\n",
    "for filename in os.listdir(\"./data/bark\"):\n",
    "  if not filename.endswith(\".wav\"): continue\n",
    "  folder_name = filename.split(\".wav\")[0]\n",
    "  originals[folder_name] = load_voice(f\"./data/bark/{filename}\")\n",
    "  if not folder_name in voice_map: voice_map[folder_name] = []\n",
    "  for fn in os.listdir(f\"./data/bark/{folder_name}\"):\n",
    "    if not fn.endswith(\".wav\"): continue\n",
    "    voice_map[folder_name].append(load_voice(f\"./data/bark/{folder_name}/{fn}\"))\n",
    "\n",
    "num_frames = MAX_DURATION // DURATION\n",
    "num_voices = sum([len(voices) for voices in voice_map.values()])\n",
    "num_samples = 5\n",
    "ptr_net = PointerNet(len(voice_map) * num_samples * num_frames)\n",
    "optimizer = optim.Adam(ptr_net.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "\n",
    "ptr_net.train()\n",
    "\n",
    "if device == \"cuda\": ptr_net.cuda()\n",
    "for step in range(1, NUM_STEPS + 1):\n",
    "  optimizer.zero_grad()\n",
    "  processed = []\n",
    "  labels = torch.zeros((1, len(voice_map) * num_samples * num_frames)).to(device)\n",
    "  for i, voices in enumerate(voice_map.values()):\n",
    "    for voice in random.choices(voices, k=num_samples):\n",
    "      start = np.random.randint(0, len(voice) - int(MAX_DURATION * SAMPLE_RATE / 1000))\n",
    "      processed.extend(decompose_voice(voice[start:], EMBEDDING_DIM, int(MAX_DURATION * SAMPLE_RATE / 1000)))\n",
    "    labels[:,i * num_frames * num_samples:] += 1\n",
    "  labels -= 1\n",
    "  outputs, pointers = ptr_net(torch.tensor(processed).to(device).view(1, len(voice_map) * num_samples * num_frames, -1))\n",
    "  loss = loss_fn(torch.floor(pointers / num_samples / num_frames), labels)\n",
    "  loss.requires_grad = True\n",
    "  # display(torch.floor(pointers * len(voice_map) / num_voices / num_frames), labels)\n",
    "  losses.append(loss.item())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  if step % 100 == 0:\n",
    "    print(f\"{step}/{NUM_STEPS}:\\t{np.mean(losses):.4f}\")\n",
    "    losses = []\n",
    "print(\"Training Ended!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptr_net.eval()\n",
    "with torch.no_grad():\n",
    "  processed = []\n",
    "  labels = torch.zeros((1, len(voice_map) * num_samples * num_frames)).to(device)\n",
    "  for i, voices in enumerate(voice_map.values()):\n",
    "    for voice in random.choices(voices, k=num_samples):\n",
    "      start = np.random.randint(0, len(voice) - int(MAX_DURATION * SAMPLE_RATE / 1000))\n",
    "      processed.extend(decompose_voice(voice[start:], EMBEDDING_DIM, int(MAX_DURATION * SAMPLE_RATE / 1000)))\n",
    "    labels[:,i * num_frames * num_samples:] += 1\n",
    "  labels -= 1\n",
    "  outputs, pointers = ptr_net(torch.tensor(processed).to(device).view(1, len(voice_map) * num_samples * num_frames, -1))\n",
    "  display(num_frames * num_samples)\n",
    "  display(pointers, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(basename, raw_prompt, index):\n",
    "  basepath = \"./data/bark/\"\n",
    "  text_prompt = re.sub(r\"\\s\\s+\", \" \", re.sub(r\"[\\t\\n]\", \"\", raw_prompt)).strip()\n",
    "  print(f\"{basename}: prompt-{index}\")\n",
    "  audio_array, generation_data = generate_audio(text_prompt, basename)\n",
    "  if basename not in os.listdir(basepath): os.mkdir(f\"{basepath}/{basename}\")\n",
    "  save_history(f\"{basepath}/{basename}/prompt-{index}\", audio_array, text_prompt, generation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-0\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 24.21it/s]\n",
      "100%|██████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-1\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.84it/s] \n",
      "100%|██████████| 19/19 [00:18<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-2\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 32.42it/s]\n",
      "100%|██████████| 18/18 [00:17<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-3\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.84it/s]\n",
      "100%|██████████| 24/24 [00:23<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-4\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.22it/s] \n",
      "100%|██████████| 14/14 [00:14<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-5\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.81it/s] \n",
      "100%|██████████| 11/11 [00:11<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-6\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 28.94it/s] \n",
      "100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-7\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.16it/s]\n",
      "100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-8\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.20it/s]\n",
      "100%|██████████| 16/16 [00:16<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman-1: prompt-9\n",
      "history_prompt in gen: woman-1\n",
      "woman-1\n",
      "aa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 59.00it/s]\n",
      "100%|██████████| 12/12 [00:11<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_prompts = [\n",
    "  \"A pessimist is one who makes difficulties of his opportunities and an optimist is one who makes opportunities of his difficulties.\",\n",
    "  \"Don't judge each day by the harvest you reap but by the seeds that you plant.\",\n",
    "  \"Challenges are what make life interesting and overcoming them is what makes life meaningful.\",\n",
    "  \"Happiness lies not in the mere possession of money; it lies in the joy of achievement, in the thrill of creative effort.\",\n",
    "  \"I disapprove of what you say, but I will defend to the death your right to say it.\",\n",
    "  \"If I looked compared to others far, is because I stand on giant's shoulder.\",\n",
    "  \"Never argue with stupid people, they will drag you down to their level and then beat you with experience.\",\n",
    "  \"The greatest glory in living lies not in never falling, but in rising every time we fall.\",\n",
    "  \"When you look into the abyss, the abyss also looks into you.\",\n",
    "  \"Whoever fights monsters should see to it that in the process he does not become a monster.\"\n",
    "]\n",
    "\n",
    "basename = \"woman-1\"\n",
    "\n",
    "for i, raw_prompt in enumerate(raw_prompts):\n",
    "  generate(basename, raw_prompt, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
